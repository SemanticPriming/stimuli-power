---
title: "Methods Power Study"
author: "Erin M. Buchanan"
date: "Last Knitted: `r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r}
set.seed(895893) # 233232
library(dplyr)
library(ggplot2)
library(tidyr)
library(broom)
library(papaja)
library(rio)
```

This document includes the informal write up for the method for the manuscript. We exported the simulations to this document in order to run the simulations separately from the manuscript for speed reasons (i.e., it's really slow to run the simulations along with the manuscript markdown). At the end of this document, a `simulated_summary_data.csv` file is created that is imported into the manuscript for plotting and other analyses. Please see the manuscript for formal write up. 

# Method

## Data Simulation 

*Population*. 

- Simulate the data with `rnorm` assuming a normal distribution for 30 scale items
- Population is created with 1000 data points 
- No rounding 
- We will run this part 100 times, so create a function that does this for you. 

```{r sim_pop, include = T, echo = T}
simulate_population <- function (x = 1){
  
  # small potential variability overall, sort of 1-7ish scale
  mu1 <- rnorm(30, 4, .25)
  sigma1.1 <- rnorm(30, 2, .2)
  sigma2.1 <- rnorm(30, 2, .4)
  sigma3.1 <- rnorm(30, 2, .8)
  
  # medium potential variability 0 to 100 scale
  mu2 <- rnorm(30, 50, 10)
  sigma1.2 <- rnorm(30, 25, 4)
  sigma2.2 <- rnorm(30, 25, 8)
  sigma3.2 <- rnorm(30, 25, 16)
  
  while(sum(sigma3.2 < 0) > 0){
    sigma3.2 <- rnorm(30, 25, 16)
  }
  
  # large potential variability in the 1000s scale
  mu3 <- rnorm(30, 1000, 150)
  sigma1.3 <- rnorm(30, 400, 50)
  sigma2.3 <- rnorm(30, 400, 100)
  sigma3.3 <- rnorm(30, 400, 200)
  
  while(sum(sigma3.3 < 0) > 0){
    sigma3.3 <- rnorm(30, 400, 200)
  }
  
  population1 <- data.frame(
    item = rep(1:30, 1000*3),
    scale = rep(1:3, each = 1000*30),
    score = c(rnorm(1000*30, mean = mu1, sd = sigma1.1),
              rnorm(1000*30, mean = mu2, sd = sigma1.2),
              rnorm(1000*30, mean = mu3, sd = sigma1.3))
    )
  
  population2 <- data.frame(
    item = rep(1:30, 1000*3),
    scale = rep(1:3, each = 1000*30),
    score = c(rnorm(1000*30, mean = mu1, sd = sigma2.1),
              rnorm(1000*30, mean = mu2, sd = sigma2.2),
              rnorm(1000*30, mean = mu3, sd = sigma2.3))
    )
  
  population3 <- data.frame(
    item = rep(1:30, 1000*3),
    scale = rep(1:3, each = 1000*30),
    score = c(rnorm(1000*30, mean = mu1, sd = sigma3.1),
              rnorm(1000*30, mean = mu2, sd = sigma3.2),
              rnorm(1000*30, mean = mu3, sd = sigma3.3))
    )
  
  # return populations 
  return(list(population1 = population1, 
              population2 = population2, 
              population3 = population3))
}
```

- Within that function, create the scale size variable: 
  - Small scale data 1-7, mean 4, sd of mean .25
  - Medium scale data 0-100, mean 50, sd of mean 10
  - Large scale data no range, mean 1000, sd of mean 150
  - These choices are meant to mimic popular scale choices 
- Additionally, create the scale heterogeneity variable:
  - Small scale heterogeneity, sd 2, sd of sd .2 .4 .8
  - Medium scale heterogeneity, sd 25, sd of sd 4, 8, 16
  - Large scale heterogeneity, sd 400, sd of sd 50, 100, 200
  
*Samples*. 

- Create a function that does the samples that a researcher might do
- Includes a start and stop of the number of participants you might consider 
- Includes an increasing value between samples
- We will also run this part 100 times 

```{r sim_sample, include = T, echo = T}
simulate_samples <- function(start = 20, stop = 100, increase = 5,
                             population1, population2, population3){
  
  # save those samples for small medium large 
  samples1 <- samples2 <- samples3 <- list() 
  
  # create the list of sizes 
  sizes <- seq(from = start, to = stop, by = increase)
  
  # loop over sizes and create those samples 
  for (i in 1:length(sizes)){
    samples1[[i]] <- population1 %>% 
      group_by(item, scale) %>% 
      slice_sample(n = sizes[i])

    samples2[[i]] <- population2 %>%
      group_by(item, scale) %>%
      slice_sample(n = sizes[i])

    samples3[[i]] <- population3 %>%
      group_by(item, scale) %>%
      slice_sample(n = sizes[i])
  }
  
  # return your values
  return(list(samples1 = samples1, 
              samples2 = samples2, 
              samples3 = samples3,
              sizes = sizes))
  
}
```

*Cutoff Score Criterions*. 

- Calculate the SEs for the items at each decile 
- Save as a function for later 

```{r calc_SE, include = T, echo = T}
calculate_deciles <- function(samples1, samples2, samples3){
  # calculate the SEs and the cutoff scores 
  SES1 <- SES2 <- SES3 <- list()
  cutoffs1 <- cutoffs2 <- cutoffs3 <- list()
  sd_items1 <- sd_items2 <- sd_items3 <- list()
  
  # loop and calculate for each sample size 
  for (i in 1:length(samples1)){
  sd_items1[[i]] <- samples1[[i]] %>% group_by(item, scale) %>% 
    summarize(sd = sd(score), .groups = "keep") %>% 
    ungroup() %>% group_by(scale) %>% summarize(sd_item = sd(sd))
  sd_items2[[i]] <- samples2[[i]] %>% group_by(item, scale) %>% 
    summarize(sd = sd(score), .groups = "keep") %>% 
    ungroup() %>% group_by(scale) %>% summarize(sd_item = sd(sd))
  sd_items3[[i]] <- samples3[[i]] %>% group_by(item, scale) %>% 
    summarize(sd = sd(score), .groups = "keep") %>% 
    ungroup() %>% group_by(scale) %>% summarize(sd_item = sd(sd))
  
  SES1[[i]] <- tapply(samples1[[i]]$score,
                     list(samples1[[i]]$item,
                          samples1[[i]]$scale),
                     function (x){ sd(x)/sqrt(length(x))})
  SES2[[i]] <- tapply(samples2[[i]]$score,
                   list(samples2[[i]]$item,
                        samples2[[i]]$scale),
                   function (x){ sd(x)/sqrt(length(x))})
  SES3[[i]] <- tapply(samples3[[i]]$score,
                 list(samples2[[i]]$item,
                      samples2[[i]]$scale),
                 function (x){ sd(x)/sqrt(length(x))})

  cutoffs1[[i]] <- apply(as.data.frame(SES1[[i]]), 2, 
                         quantile, 
                         probs = seq(0, .9, by = .1),
                         na.rm = T)
  cutoffs2[[i]] <- apply(as.data.frame(SES2[[i]]), 2, 
                         quantile, 
                         probs = seq(0, .9, by = .1),
                         na.rm = T)
  cutoffs3[[i]] <- apply(as.data.frame(SES3[[i]]), 2, 
                         quantile, 
                         probs = seq(0, .9, by = .1),
                         na.rm = T)
  
  }
  
  # return information
  return(list(SES1 = SES1, SES2 = SES2, SES3 = SES3, 
              cutoffs1 = cutoffs1, cutoffs2 = cutoffs2, cutoffs3 = cutoffs3, 
              sd_items1 = sd_items1, sd_items2 = sd_items2, sd_items3 = sd_items3))

}
```

## Researcher Sample Simulation 

- Here we are simulating the researcher side of the equation assuming they have a sample pulled from population (which we simulated above)
- We will simulate samples from 20 to 2000 ... mostly only to see the leveling out effect of the simulation procedure, as we likely believe that over 2000 is not reasonable for most researchers. 
- Calculate the SEs here, to see if it matches our cutoff 
- Save this as a function to run within our pipeline (100 populations and 100 samples)

```{r sim_sim, include = T, echo = T}
simulate_researcher <- function(samples1, samples2, samples3, 
                                start = 20, stop = 2000, 
                                increase = 5){
  
  # sequence of sample sizes to try
  samplesize_values <- seq(from = start, to = stop, by = increase)

  # place to store everything
  sampled_values1 <- sampled_values2 <- sampled_values3 <- list()

  # loop over the samples
  for (i in 1:length(samples1)){
    
    # create a blank table for us to save the values in 
    sim_table1 <- matrix(NA, 
                        nrow = length(samplesize_values), 
                        ncol = 30*3)
    
    # make it a data frame
    sim_table1 <- sim_table2 <- sim_table3 <- as.data.frame(sim_table1)
    
    # add a place for sample size values 
    sim_table1$sample_size <- sim_table2$sample_size <- sim_table3$sample_size <- NA
    
    # loop over pilot sample sizes
    for (q in 1:length(samplesize_values)){
        
      # temp dataframe that samples and summarizes
      temp <- samples1[[i]] %>% 
        group_by(item, scale) %>% 
        slice_sample(n = samplesize_values[q], replace = T) %>% 
        summarize(se = sd(score)/sqrt(length(score)),
                  .groups = "keep")
      
      sim_table1[q, 1:90] <- temp$se
      sim_table1[q, 91] <- samplesize_values[q]
      
      temp <- samples2[[i]] %>% 
        group_by(item, scale) %>% 
        slice_sample(n = samplesize_values[q], replace = T) %>% 
        summarize(se = sd(score)/sqrt(length(score)),
                  .groups = "keep")
      
      sim_table2[q, 1:90] <- temp$se
      sim_table2[q, 91] <- samplesize_values[q]
      
      temp <- samples3[[i]] %>% 
        group_by(item, scale) %>% 
        slice_sample(n = samplesize_values[q], replace = T) %>% 
        summarize(se = sd(score)/sqrt(length(score)),
                  .groups = "keep")
      
      sim_table3[q, 1:90] <- temp$se
      sim_table3[q, 91] <- samplesize_values[q]
      
      } # end pilot sample loop 
    
    sampled_values1[[i]] <- sim_table1
    sampled_values2[[i]] <- sim_table2
    sampled_values3[[i]] <- sim_table3

  } # end all sample loop
  
  # return sampled values 
  return(list(sampled_values1 = sampled_values1, 
              sampled_values2 = sampled_values2, 
              sampled_values3 = sampled_values3,
              samplesize_values = samplesize_values))
  
}
```

- Next, calculate the percent of items falling below the decile scores 
- Pick up 80, 85, 90, and 95% of items to mimic power 
- Save a function for our pipeline 

```{r calc_percent, include = T, echo = T}
calculate_percent <- function(sampled_values1, sampled_values2, sampled_values3,
                              cutoffs1, cutoffs2, cutoffs3, 
                              samplesize_values, sizes){
  
  # create temporary storage
  summary_list1 <- summary_list2 <- summary_list3 <- list()
  
  # loop and calculate
  for (i in 1:length(sampled_values1)){
  
    # summary list 1 ----
    summary_list1[[i]] <- sampled_values1[[i]] %>% 
      pivot_longer(cols = -c(sample_size)) %>% 
      rename(item = name, se = value) %>% 
      mutate(scale = rep(1:3, 30*length(samplesize_values))) %>% 
      mutate(item = rep(rep(1:30, each = 3), length(samplesize_values))) 
      
    # cut offs for 1
    temp1.1 <- summary_list1[[i]] %>% 
      filter(scale == "1") %>% 
      group_by(sample_size, scale) %>% 
      summarize(Percent_Below0 = sum(se <= cutoffs1[[i]][1, 1])/30,
             Percent_Below10 = sum(se <= cutoffs1[[i]][2, 1])/30,
             Percent_Below20 = sum(se <= cutoffs1[[i]][3, 1])/30,
             Percent_Below30 = sum(se <= cutoffs1[[i]][4, 1])/30,
             Percent_Below40 = sum(se <= cutoffs1[[i]][5, 1])/30,
             Percent_Below50 = sum(se <= cutoffs1[[i]][6, 1])/30, 
             Percent_Below60 = sum(se <= cutoffs1[[i]][7, 1])/30, 
             Percent_Below70 = sum(se <= cutoffs1[[i]][8, 1])/30, 
             Percent_Below80 = sum(se <= cutoffs1[[i]][9, 1])/30, 
             Percent_Below90 = sum(se <= cutoffs1[[i]][10, 1])/30, 
             .groups = "keep") %>% 
      mutate(original_n = sizes[i], 
             source = "low")
      
    temp1.2 <- summary_list1[[i]] %>% 
      filter(scale == "2") %>% 
      group_by(sample_size, scale) %>% 
      summarize(Percent_Below0 = sum(se <= cutoffs1[[i]][1, 2])/30,
             Percent_Below10 = sum(se <= cutoffs1[[i]][2, 2])/30,
             Percent_Below20 = sum(se <= cutoffs1[[i]][3, 2])/30,
             Percent_Below30 = sum(se <= cutoffs1[[i]][4, 2])/30,
             Percent_Below40 = sum(se <= cutoffs1[[i]][5, 2])/30,
             Percent_Below50 = sum(se <= cutoffs1[[i]][6, 2])/30, 
             Percent_Below60 = sum(se <= cutoffs1[[i]][7, 2])/30, 
             Percent_Below70 = sum(se <= cutoffs1[[i]][8, 2])/30, 
             Percent_Below80 = sum(se <= cutoffs1[[i]][9, 2])/30, 
             Percent_Below90 = sum(se <= cutoffs1[[i]][10, 2])/30, 
             .groups = "keep") %>% 
      mutate(original_n = sizes[i],
             source = "low")
    
    temp1.3 <- summary_list1[[i]] %>% 
      filter(scale == "3") %>% 
      group_by(sample_size, scale) %>% 
      summarize(Percent_Below0 = sum(se <= cutoffs1[[i]][1, 3])/30,
             Percent_Below10 = sum(se <= cutoffs1[[i]][2, 3])/30,
             Percent_Below20 = sum(se <= cutoffs1[[i]][3, 3])/30,
             Percent_Below30 = sum(se <= cutoffs1[[i]][4, 3])/30,
             Percent_Below40 = sum(se <= cutoffs1[[i]][5, 3])/30,
             Percent_Below50 = sum(se <= cutoffs1[[i]][6, 3])/30, 
             Percent_Below60 = sum(se <= cutoffs1[[i]][7, 3])/30, 
             Percent_Below70 = sum(se <= cutoffs1[[i]][8, 3])/30, 
             Percent_Below80 = sum(se <= cutoffs1[[i]][9, 3])/30, 
             Percent_Below90 = sum(se <= cutoffs1[[i]][10, 3])/30, 
             .groups = "keep") %>% 
      mutate(original_n = sizes[i], 
             source = "low")
    
    #rejoin 
    summary_list1[[i]] <- bind_rows(temp1.1, temp1.2, temp1.3)
    
    # summary list 2 ----
    summary_list2[[i]] <- sampled_values2[[i]] %>% 
      pivot_longer(cols = -c(sample_size)) %>% 
      rename(item = name, se = value) %>% 
      mutate(scale = rep(1:3, 30*length(samplesize_values))) %>% 
      mutate(item = rep(rep(1:30, each = 3), length(samplesize_values)))
    
    # cut offs for 2
    temp2.1 <- summary_list2[[i]] %>% 
      filter(scale == "1") %>% 
      group_by(sample_size, scale) %>% 
      summarize(Percent_Below0 = sum(se <= cutoffs2[[i]][1, 1])/30,
             Percent_Below10 = sum(se <= cutoffs2[[i]][2, 1])/30,
             Percent_Below20 = sum(se <= cutoffs2[[i]][3, 1])/30,
             Percent_Below30 = sum(se <= cutoffs2[[i]][4, 1])/30,
             Percent_Below40 = sum(se <= cutoffs2[[i]][5, 1])/30,
             Percent_Below50 = sum(se <= cutoffs2[[i]][6, 1])/30, 
             Percent_Below60 = sum(se <= cutoffs2[[i]][7, 1])/30, 
             Percent_Below70 = sum(se <= cutoffs2[[i]][8, 1])/30, 
             Percent_Below80 = sum(se <= cutoffs2[[i]][9, 1])/30, 
             Percent_Below90 = sum(se <= cutoffs2[[i]][10, 1])/30, 
             .groups = "keep") %>% 
      mutate(original_n = sizes[i],
             source = "med")
      
    temp2.2 <- summary_list2[[i]] %>% 
      filter(scale == "2") %>% 
      group_by(sample_size, scale) %>% 
      summarize(Percent_Below0 = sum(se <= cutoffs2[[i]][1, 2])/30,
             Percent_Below10 = sum(se <= cutoffs2[[i]][2, 2])/30,
             Percent_Below20 = sum(se <= cutoffs2[[i]][3, 2])/30,
             Percent_Below30 = sum(se <= cutoffs2[[i]][4, 2])/30,
             Percent_Below40 = sum(se <= cutoffs2[[i]][5, 2])/30,
             Percent_Below50 = sum(se <= cutoffs2[[i]][6, 2])/30, 
             Percent_Below60 = sum(se <= cutoffs2[[i]][7, 2])/30, 
             Percent_Below70 = sum(se <= cutoffs2[[i]][8, 2])/30, 
             Percent_Below80 = sum(se <= cutoffs2[[i]][9, 2])/30, 
             Percent_Below90 = sum(se <= cutoffs2[[i]][10, 2])/30, 
             .groups = "keep") %>% 
      mutate(original_n = sizes[i], 
             source = "med")
    
    temp2.3 <- summary_list2[[i]] %>% 
      filter(scale == "3") %>% 
      group_by(sample_size, scale) %>% 
      summarize(Percent_Below0 = sum(se <= cutoffs2[[i]][1, 3])/30,
             Percent_Below10 = sum(se <= cutoffs2[[i]][2, 3])/30,
             Percent_Below20 = sum(se <= cutoffs2[[i]][3, 3])/30,
             Percent_Below30 = sum(se <= cutoffs2[[i]][4, 3])/30,
             Percent_Below40 = sum(se <= cutoffs2[[i]][5, 3])/30,
             Percent_Below50 = sum(se <= cutoffs2[[i]][6, 3])/30, 
             Percent_Below60 = sum(se <= cutoffs2[[i]][7, 3])/30, 
             Percent_Below70 = sum(se <= cutoffs2[[i]][8, 3])/30, 
             Percent_Below80 = sum(se <= cutoffs2[[i]][9, 3])/30, 
             Percent_Below90 = sum(se <= cutoffs2[[i]][10, 3])/30, 
             .groups = "keep") %>% 
      mutate(original_n = sizes[i],
             source = "med")
    
    #rejoin 
    summary_list2[[i]] <- bind_rows(temp2.1, temp2.2, temp2.3)
    
    # summary list 3 ----
    summary_list3[[i]] <- sampled_values3[[i]] %>% 
      pivot_longer(cols = -c(sample_size)) %>% 
      rename(item = name, se = value) %>% 
      mutate(scale = rep(1:3, 30*length(samplesize_values))) %>% 
      mutate(item = rep(rep(1:30, each = 3), length(samplesize_values)))
    
    # cut offs for 3 
    temp3.1 <- summary_list3[[i]] %>% 
      filter(scale == "1") %>% 
      group_by(sample_size, scale) %>% 
      summarize(Percent_Below0 = sum(se <= cutoffs3[[i]][1, 1])/30,
             Percent_Below10 = sum(se <= cutoffs3[[i]][2, 1])/30,
             Percent_Below20 = sum(se <= cutoffs3[[i]][3, 1])/30,
             Percent_Below30 = sum(se <= cutoffs3[[i]][4, 1])/30,
             Percent_Below40 = sum(se <= cutoffs3[[i]][5, 1])/30,
             Percent_Below50 = sum(se <= cutoffs3[[i]][6, 1])/30, 
             Percent_Below60 = sum(se <= cutoffs3[[i]][7, 1])/30, 
             Percent_Below70 = sum(se <= cutoffs3[[i]][8, 1])/30, 
             Percent_Below80 = sum(se <= cutoffs3[[i]][9, 1])/30, 
             Percent_Below90 = sum(se <= cutoffs3[[i]][10, 1])/30, 
             .groups = "keep") %>% 
      mutate(original_n = sizes[i],
             source = "high")
       
    temp3.2 <- summary_list3[[i]] %>% 
      filter(scale == "2") %>% 
      group_by(sample_size, scale) %>% 
      summarize(Percent_Below0 = sum(se <= cutoffs3[[i]][1, 2])/30,
             Percent_Below10 = sum(se <= cutoffs3[[i]][2, 2])/30,
             Percent_Below20 = sum(se <= cutoffs3[[i]][3, 2])/30,
             Percent_Below30 = sum(se <= cutoffs3[[i]][4, 2])/30,
             Percent_Below40 = sum(se <= cutoffs3[[i]][5, 2])/30,
             Percent_Below50 = sum(se <= cutoffs3[[i]][6, 2])/30, 
             Percent_Below60 = sum(se <= cutoffs3[[i]][7, 2])/30, 
             Percent_Below70 = sum(se <= cutoffs3[[i]][8, 2])/30, 
             Percent_Below80 = sum(se <= cutoffs3[[i]][9, 2])/30, 
             Percent_Below90 = sum(se <= cutoffs3[[i]][10, 2])/30, 
             .groups = "keep") %>% 
      mutate(original_n = sizes[i],
             source = "high")
    
    temp3.3 <- summary_list3[[i]] %>% 
      filter(scale == "3") %>% 
      group_by(sample_size, scale) %>% 
      summarize(Percent_Below0 = sum(se <= cutoffs3[[i]][1, 3])/30,
             Percent_Below10 = sum(se <= cutoffs3[[i]][2, 3])/30,
             Percent_Below20 = sum(se <= cutoffs3[[i]][3, 3])/30,
             Percent_Below30 = sum(se <= cutoffs3[[i]][4, 3])/30,
             Percent_Below40 = sum(se <= cutoffs3[[i]][5, 3])/30,
             Percent_Below50 = sum(se <= cutoffs3[[i]][6, 3])/30, 
             Percent_Below60 = sum(se <= cutoffs3[[i]][7, 3])/30, 
             Percent_Below70 = sum(se <= cutoffs3[[i]][8, 3])/30, 
             Percent_Below80 = sum(se <= cutoffs3[[i]][9, 3])/30, 
             Percent_Below90 = sum(se <= cutoffs3[[i]][10, 3])/30, 
             .groups = "keep") %>% 
      mutate(original_n = sizes[i],
             source = "high")
    
    #rejoin 
    summary_list3[[i]] <- bind_rows(temp3.1, temp3.2, temp3.3)
  
    } # end loop and calculate 
  
  # create end summary 
  summary_DF <- bind_rows(summary_list1, 
                        summary_list2, 
                        summary_list3)
  
  # return values 
  return(summary_DF)
  
}
```

- Last combine everything together grabbing the proposed sample at each decile given the 80, 85, 90, and 95 scores 
- Create a function for our pipeline 

```{r calc_percent_combine, include = T, echo = T}
calculate_proposed <- function(summary_DF){
  
  # 80% summary 
  summary_long_80 <- summary_DF %>% 
    pivot_longer(cols = -c(sample_size, original_n, source, scale)) %>% 
    filter(value >= .80) %>% 
    arrange(sample_size, original_n, source, scale, name) %>% 
    group_by(original_n, name, source, scale) %>% 
    slice_head(n = 1) %>% 
    mutate(power = 80)
  
  # 85% summary
  summary_long_85 <- summary_DF %>% 
    pivot_longer(cols = -c(sample_size, original_n, source, scale)) %>% 
    filter(value >= .85) %>% 
    arrange(sample_size, original_n, source, scale, name) %>% 
    group_by(original_n, name, source, scale) %>% 
    slice_head(n = 1) %>% 
    mutate(power = 85)
  
  # 90% summary  
  summary_long_90 <- summary_DF %>% 
    pivot_longer(cols = -c(sample_size, original_n, source, scale)) %>% 
    filter(value >= .90) %>% 
    arrange(sample_size, original_n, source, scale, name) %>% 
    group_by(original_n, name, source, scale) %>% 
    slice_head(n = 1) %>% 
    mutate(power = 90)
  
  # 95% summary
  summary_long_95 <- summary_DF %>% 
    pivot_longer(cols = -c(sample_size, original_n, source, scale)) %>% 
    filter(value >= .95) %>% 
    arrange(sample_size, original_n, source, scale, name) %>% 
    group_by(original_n, name, source, scale) %>% 
    slice_head(n = 1) %>% 
    mutate(power = 95)
  
  # combine the summary together
  summary_long <- rbind(summary_long_80, 
                        summary_long_85,
                        summary_long_90,
                        summary_long_95)
  
  # return values
  return(summary_long)
  
}
```

## Simulation Pipeline

- Put together all the simulation pieces 
  - Simulate the population 
  - Simulate samples within that population 
  - Calculate the cutoffs and deciles 
  - Simulate like the researcher would 
  - Calculate the percent under the decile
  - Create final summary 

- We will simulate multiple times at each simulation point:
  - Samples X 100
  - Simulate Researcher X 100

```{r simulation_pipeline, include = T, echo = T}
# a list to save this in 
full_simulation <- list()
populations <- simulate_population(x)
    
for (p in 1:100){
  
  # simulate samples from population
  samples <- simulate_samples(start = 20, stop = 100, increase = 10, 
                          population1 = populations$population1,
                          population2 = populations$population2, 
                          population3 = populations$population3)
  
  # calculate deciles 
  deciles <- calculate_deciles(samples1 = samples$samples1, 
                           samples2 = samples$samples2,
                           samples3 = samples$samples3)
  
  cat(paste(p, Sys.time(), "\n"))
  
  # reset this with each new sample 
  researcher_summary <- list()
  
  for (q in 1:100){
    
    # simulate researcher
    researcher_data <- simulate_researcher(samples1 = samples$samples1, 
                                     samples2 = samples$samples2,
                                     samples3 = samples$samples3, 
                                     start = 20, stop = 2000, 
                                     increase = 20)
    
    # calculate percent under each decile
    percent_items <- calculate_percent(
      sampled_values1 = researcher_data$sampled_values1, 
      sampled_values2 = researcher_data$sampled_values2, 
      sampled_values3 = researcher_data$sampled_values3, 
      cutoffs1 = deciles$cutoffs1, 
      cutoffs2 = deciles$cutoffs2, 
      cutoffs3 = deciles$cutoffs3, 
      samplesize_values = researcher_data$samplesize_values,
      sizes = samples$sizes)
  
    # calculate final summary data 
    researcher_summary[[q]] <- calculate_proposed(summary_DF = percent_items)
    
  } # researcher 100 simulations 
  
  # after the researcher summary, summarize and stick in dataframe 
  # so we have a dataframe of each population by sample 
  full_simulation[[p]] <-
   bind_rows(researcher_summary) %>%
   group_by(scale, original_n, source, name, power) %>%
   summarize(sample_size = mean(sample_size),
             value = mean(value), .groups = "keep")
  
  # clean up the values from the summary while you have the data 
  full_simulation[[p]]$source <- factor(full_simulation[[p]]$source, 
                                levels = c("low", "med", "high"),
                                labels = c("Low Variance", 
                                           "Medium Variance", 
                                           "High Variance"))
  
  full_simulation[[p]]$scale2 <- factor(full_simulation[[p]]$scale, 
                                levels = c(1:3),
                                labels = c("Small Scale", 
                                           "Medium Scale", 
                                           "Large Scale"))
  
  # grab the sd information from the pilot sample
  # note this is not the researcher simulated sample 
  sd_items <- bind_rows(deciles$sd_items1, deciles$sd_items2, deciles$sd_items3)
  sd_items$source <- rep(c("Low Variance", 
                           "Medium Variance", 
                           "High Variance"), 
                         each = 3*length(samples$sizes))
  sd_items$original_n <- rep(rep(samples$sizes, each = 3), 3)
  
  # add together that information 
  full_simulation[[p]] <- full_simulation[[p]] %>% 
    full_join(sd_items, 
              by = c("original_n" = "original_n", 
                     "scale" = "scale", 
                     "source" = "source"))
  
  full_simulation[[p]]$name <- gsub("Percent_Below", "Decile ", full_simulation[[p]]$name)
  
  # writing out takes time but don't want to lose 100 researcher sims
  # ran this across computers so make clear when run 
  saveRDS(full_simulation, file = paste("simulation_", Sys.time(), ".Rdata", sep = ""))

} # simulate sampling 100 times from the population
```

## Put Together Data

```{r}
sim_mac <- readRDS("simulation_2022-08-14 12:54:44.Rdata")
sim_win
sim_linux

all_sim <- bind_rows(sim_mac, .id = "id")

summary_long <- all_sim %>% 
  group_by(scale, original_n, source, name, power) %>% 
  summarize(sample_size = mean(sample_size),
             value = mean(value), .groups = "keep")
```

## Export Final Data

```{r}

```
