---
title: "Attractiveness Ratings Example"
author: "Erin M. Buchanan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Set a random see
set.seed(5989320)

# Libraries necessary for this vignette
library(rio)
library(flextable)
library(dplyr)
library(tidyr)
library(psych)
```

### Project/Data Title: 

Attractiveness Ratings 

Data provided by: Carlota Batres 

### Project/Data Description: 

This dataset contains 200 participants rating 20 faces on attractiveness. Ethical approval was received from the Franklin and Marshall Institutional Review Board and each participant provided informed consent. All participants were located in the United States. Participants were instructed that they would be viewing several faces which were photographed facing forward, under constant camera and lighting conditions, with neutral expressions, and closed mouths. Each participant would have to rate the attractiveness of the presented faces. More specifically, participants were asked "How attractive is this face?", where 1 = "Not at all attractive" and 7 = "Very attractive". Participants rated each face individually, in random order, and with no time limit. Upon completion, participants were paid for participation in the study.

### Methods Description: 

The data was collected online using Amazonâ€™s Mechanical Turk platform.  

### Data Location: 

Included with the vignette. 

```{r}
DF <- import("batres_data.sav")

str(DF)
```

### Date Published: 

No official publication date.

### Dataset Citation: 

Batres, C. (2022). Attractiveness Ratings. [Data set]. 

### Keywords: 

faces, ratings

### Use License: 

Attribution-NonCommercial-ShareAlike CC BY-NC-SA

### Geographic Description - City/State/Country of Participants:

United States

### Column Metadata: 

```{r}
metadata <- import("batres_metadata.xlsx")

flextable(metadata) %>% autofit()
```

### AIPE Analysis 

```{r}
# Reformat the data
DF_long <- pivot_longer(DF, cols = -c(Participant_Number)) %>% 
  rename(item = name, score = value)

flextable(head(DF_long)) %>% autofit()
```

#### Stopping Rule

What the usual standard error for the data that could be considered for our stopping rule using the 50% decile? 

```{r}
# individual SEs
SE <- tapply(DF_long$score, DF_long$item, function (x) { sd(x)/sqrt(length(x)) })

SE

cutoff <- quantile(SE, probs = .50)
cutoff
```

Using our 50% decile as a guide, we find that `r round(cutoff, digits = 3)` is our target standard error for an accurately measured item. 

#### Minimum Sample Size

To estimate minimum sample size, we should figure out what number of participants it would take to achieve 80%, 85%, 90%, and 95% of the SEs for items below our critical score of `r round(cutoff, digits = 3)`?

Please note that you will always need to simulate larger than the pilot data sample size to get the starting numbers. We will correct them below. 

```{r}
# sequence of sample sizes to try
samplesize_values <- seq(20, 300, 5)

# create a blank table for us to save the values in 
sim_table <- matrix(NA, 
                    nrow = length(samplesize_values), 
                    ncol = length(unique(DF_long$item)))

# make it a data frame
sim_table <- as.data.frame(sim_table)

# add a place for sample size values 
sim_table$sample_size <- NA

# loop over sample sizes
for (i in 1:length(samplesize_values)){
    
  # temp dataframe that samples and summarizes
  temp <- DF_long %>% 
    group_by(item) %>% 
    sample_n(samplesize_values[i], replace = T) %>% 
    summarize(se = sd(score)/sqrt(length(score))) 
  
  colnames(sim_table)[1:length(unique(DF_long$item))] <- temp$item
  sim_table[i, 1:length(unique(DF_long$item))] <- temp$se
  sim_table[i, "sample_size"] <- samplesize_values[i]
  }

# figure out cut off
final_sample <- 
  sim_table %>% 
  pivot_longer(cols = -c(sample_size)) %>% 
  rename(item = name, se = value) %>% 
  group_by(sample_size) %>% 
  summarize(Percent_Below = sum(se <= cutoff)/length(unique(DF_long$item))) 
```

As shown in our manuscript, we need to correct for the overestimation of sample sizes based on the original pilot data size. Given that the pilot data is large: `r nrow(DF)`, this correction is especially useful. 

$N_{Corrected Projected} = 39.269 + 0.700 \times X_{N_{Projected}} + 0.003 \times X_{SD Items} - 0.694 \times X_{N_{Pilot}}$

```{r}
# calculate the SD of the item's SD 
sd_items <- sd(tapply(DF_long$score, DF_long$item, sd))
sd_items
                   
final_sample$new_sample <- round(39.369 + 0.700*final_sample$sample_size + 0.003*sd_items - 0.694*nrow(DF))

flextable(final_sample %>% 
            filter(Percent_Below >= .80) %>% 
            arrange(Percent_Below, new_sample)) %>% 
  autofit()
```

Our minimum suggested sample size does not exist at exactly 80% of the items, but instead we can use 85% (*n* = 62 as the minimum). In this example, 90% actually hits below 85% at the minimum (*n* = 58). For 95% of items hitting criterion, the minimum sample size would be approximately 83. In this case, the researcher could decide that 62 would be their minimum sample size. 

#### Maximum Sample Size

While there are many considerations for maximum sample size (time, effort, resources), the simulation suggests that 83 people would ensure nearly all items achieve cutoff criterions. 

#### Final Sample Size

In any estimate for sample size, you should also consider the potential for missing data and/or unusable data due to any other exclusion criteria in your study (i.e., attention checks, speeding, getting the answer right, etc.). In this study, we likely expect all participants to see all items and therefore, we could expect to use the minimum sample size as our final sample size, the point at which all items reach our SE criterion, or the maximum sample size. 

