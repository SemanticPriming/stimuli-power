---
title: "Survival processing usefulness"
author: "Necdet Gürkan & Yangyang Yu"
date: "`r Sys.Date()`"
output: word_document
  # html_document:
  #   toc: true
  #   toc_depth: 4
  #   toc_float: true
---
### Vignette Setup: 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Libraries necessary for this vignette
library(rio)
library(flextable)
library(dplyr)
library(tidyr)
```

### Project/Data Title: 

The survival processing effect

Data provided by: Röer, Bell & Buchner (2013)

### Project/Data Description: 

The data come from a conceptual replication study on the survival processing effect. The survival processing effect refers to the finding that rating words according to their relevance in a survival-related scenario leads to better retention than processing words in a number of other fictional scenarios. Participants were randomly assigned to one of the rating scenarios (survival, afterlife, moving). The to-be-rated words were presented individually in a random order on the computer screen. Each word remained on the screen for five seconds. Participants rated the words by clicking on a 5-point scale that ranged from completely useless (1) to very useful (5), which was displayed right below the word.

### Methods Description: 

Participants were students at Heinrich-Heine-Universität Düsseldorf, Germany that were paid for participating or received course credit. Their ages ranged from 18 to 55 years. The words to-be-rated consisted of 30 typical members of 30 categories drawn from the updated Battig and Montague norms (Van Overschelde, Rawson, & Dunlosky, 2004).

### Data Location: 

Data included within this vignette. We drop the scenario column because the standard deviation and mean of item ratings across the scenarios identical. We also add a participant column to keep this script similar to other ones.

```{r}
DF <- import("roer_data.xlsx")
drops <- c("Scenario")
DF <- DF[ , !(names(DF) %in% drops)]
DF <- cbind(Participant_Number = 1:nrow(DF) , DF)

str(DF)
```

### Date Published: 

No official publication, see citation below.  

### Dataset Citation: 

Röer, J. P., Bell, R., & Buchner, A. (2013). Is the survival-processing memory advantage due to richness of encoding? Journal of Experimental Psychology: Learning, Memory, and Cognition, 39, 1294-1302.

### Keywords: 

usefulness; survival processing; adaptive memory; richness of encoding

### Use License: 

CC BY-NC

### Geographic Description - City/State/Country of Participants:

Düsseldorf, Germany

### Column Metadata:

```{r}
metadata <- import("roer_metadata.xlsx")

flextable(metadata) %>% autofit()
```

### AIPE Analysis: 

```{r}
DF_long <- pivot_longer(DF, cols = -c(Participant_Number)) %>% 
  dplyr:: rename(item = name, score = value)

flextable(head(DF_long)) %>% autofit()
```



#### Stopping Rule

What the usual standard error for the data that could be considered for our stopping rule using the 50% decile? 

```{r}
# individual SEs
SE <- tapply(DF_long$score, DF_long$item, function (x) { sd(x)/sqrt(length(x)) })

SE

cutoff <- quantile(SE, probs = .50)
cutoff
```

Using our 50% decile as a guide, we find that `r round(cutoff, digits = 3)` is our target standard error for an accurately measured item. 

#### Minimum Sample Size

To estimate minimum sample size, we should figure out what number of participants it would take to achieve 80%, 85%, 90%, and 95% of the SEs for items below our critical score of `r round(cutoff, digits = 3)`?

```{r}
# sequence of sample sizes to try
samplesize_values <- seq(20, 300, 5)

# create a blank table for us to save the values in 
sim_table <- matrix(NA, 
                    nrow = length(samplesize_values), 
                    ncol = length(unique(DF_long$item)))

# make it a data frame
sim_table <- as.data.frame(sim_table)

# add a place for sample size values 
sim_table$sample_size <- NA

# loop over sample sizes
for (i in 1:length(samplesize_values)){
    
  # temp dataframe that samples and summarizes
  temp <- DF_long %>% 
    group_by(item) %>% 
    sample_n(samplesize_values[i], replace = T) %>% 
    summarize(se = sd(score)/sqrt(length(score))) 
  
  colnames(sim_table)[1:length(unique(DF_long$item))] <- temp$item
  sim_table[i, 1:length(unique(DF_long$item))] <- temp$se
  sim_table[i, "sample_size"] <- samplesize_values[i]
  }

final_sample <- 
  sim_table %>% 
  pivot_longer(cols = -c(sample_size)) %>% 
  dplyr::rename(item = name, se = value) %>% 
  group_by(sample_size) %>% 
  summarize(Percent_Below = sum(se <= cutoff)/length(unique(DF_long$item))) %>% 
  filter(Percent_Below >= .80) %>% 
  mutate(new_sample = round(39.369 + 0.700*sample_size + 0.003*cutoff - 0.694*length(unique(DF_long$item))))

flextable(final_sample) %>% autofit()               
```

Based on these simulations, we can decide our minimum sample size is likely close to `r min(final_sample$new_sample)`.

#### Maximum Sample Size

In this example, we could set our maximum sample size for 90% power, which would equate to `r final_sample %>% filter(Percent_Below >= .90) %>% pull(new_sample) %>% min()` participants. 

#### Final Sample Size

We should consider any data loss or other issues related to survival processing when thinking about the final sample size requirements. 



