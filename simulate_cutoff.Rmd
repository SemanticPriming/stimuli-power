---
title: "Simulate Cut Off"
author: "Erin M. Buchanan"
date: "Last Knitted: `r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r}
set.seed(895893)
library(dplyr)
library(ggplot2)
library(tidyr)
library(broom)
library(papaja)
```

# Introduction

More real introduction on power, current trends to AIPE, and why items are key. 

This document outlines using the suggested procedure to simulate the number of participants necessary to accurately measure items. There are two key issues these ideas should address that we know about power:

1) we should see differences in projected sample sizes based on the variability in the variance for those items (i.e., heterogeneity should increase projected sample size)
2) we should see projected sample sizes that "level off" when pilot data increases. As with regular power estimates, studies can be "overpowered" to detect an effect, and this same idea should be present. For example, if one has a 500 person pilot study, our simulations should suggest a point at which items are likely measured well, which may have happened well before 500. 

# Explain the Procedure 

Notes forthcoming. 

# Method

## Data Simulation 

*Population*. The data was simulated using the `rnorm` function assuming a normal distribution for 30 scale type items. Each population was simulated with 1000 data points. No items were rounded for this simulation. 

First, the scale of the data was manipulated by creating three sets of scales. The first scale was mimicked after small rating scales (i.e., 1-7 type style) using a $\mu$ = 4 with a $\sigma$ = .25 around the mean to create item mean variability. The second scale included a larger potential distribution of scores with a $\mu$ = 50 ($\sigma$ = 10) imitating a 0-100 scale. Last, the final scale included a $\mu$ = 1000 ($\sigma$ = 150) simulating a study that may include response latency data in the milliseconds. While there are many potential scales, these three represent a large number of potential variables in the social sciences. As we are suggesting variances as a key factor for estimating sample sizes, the scale of the data is influential on the amount of *potential* variance. Smaller ranges of data (1-7) cannot necessarily have the same variance as larger ranges (0-100). 

Next, item variance heterogeneity was included by manipulating the potential $\sigma$ for each individual item. For small scales, the $\sigma$ = 2 points with a variability of .2, .4, and .8 for low, medium, and high heterogeneity in the variances between items. For the medium scale of data, $\sigma$ = 20 with a variance of 4, 8, and 12. Last, for the large scale of data, $\sigma$ = 350 with a variance of 100, 150, and 200 for heterogeneity. 

```{r echo = F}
# small potential variability overall, sort of 1-7ish scale
mu1 <- rnorm(30, 4, .25)
sigma1.1 <- rnorm(30, 2, .2)
sigma2.1 <- rnorm(30, 2, .4)
sigma3.1 <- rnorm(30, 2, .8)

# medium potential variability 0 to 100 scale
mu2 <- rnorm(30, 50, 10)
sigma1.2 <- rnorm(30, 20, 4)
sigma2.2 <- rnorm(30, 20, 8)
sigma3.2 <- rnorm(30, 20, 12)

# large potential variability in the 1000s scale

mu3 <- rnorm(30, 1000, 150)
sigma1.3 <- rnorm(30, 350, 100)
sigma2.3 <- rnorm(30, 350, 150)
sigma3.3 <- rnorm(30, 350, 200)

while(sum(sigma3.3 < 0) > 0){
  sigma3.3 <- rnorm(30, 350, 200)
}

population1 <- data.frame(
  item = rep(1:30, 1000*3),
  scale = rep(1:3, each = 1000*30),
  score = c(rnorm(1000*30, mean = mu1, sd = sigma1.1),
            rnorm(1000*30, mean = mu2, sd = sigma1.2),
            rnorm(1000*30, mean = mu3, sd = sigma1.3))
  )

population2 <- data.frame(
  item = rep(1:30, 1000*3),
  scale = rep(1:3, each = 1000*30),
  score = c(rnorm(1000*30, mean = mu1, sd = sigma2.1),
            rnorm(1000*30, mean = mu2, sd = sigma2.2),
            rnorm(1000*30, mean = mu3, sd = sigma2.3))
  )

population3 <- data.frame(
  item = rep(1:30, 1000*3),
  scale = rep(1:3, each = 1000*30),
  score = c(rnorm(1000*30, mean = mu1, sd = sigma3.1),
            rnorm(1000*30, mean = mu2, sd = sigma3.2),
            rnorm(1000*30, mean = mu3, sd = sigma3.3))
  )

#evidence that they are simulated correctly
tapply(population1$score, list(population1$item, 
                               population1$scale), mean)
tapply(population1$score, list(population1$item,
                               population1$scale), sd) 

tapply(population2$score, list(population2$item, 
                               population2$scale), mean)
tapply(population2$score, list(population2$item,
                               population2$scale), sd) 

tapply(population3$score, list(population3$item, 
                               population3$scale), mean)
tapply(population3$score, list(population3$item,
                               population3$scale), sd) 
```

*Samples*. Each population was then sampled as if a researcher was conducting a pilot study. The sample sizes started at 20 participants per item increasing in units of 5 up to 100 participants. 

```{r}
# create pilot samples from 20 to 100
samples1 <- samples2 <- samples3 <- list() 
sizes <- c(20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100)
for (i in 1:length(sizes)){
  samples1[[i]] <- population1 %>% 
    group_by(item, scale) %>% 
    slice_sample(n = sizes[i])
  
  samples2[[i]] <- population2 %>% 
    group_by(item, scale) %>% 
    slice_sample(n = sizes[i])
    
  samples3[[i]] <- population3 %>% 
    group_by(item, scale) %>% 
    slice_sample(n = sizes[i])
}
```

*AIPE Criterions*. The standard errors of each item were calculated to mimic the AIPE procedure of finding an appropriately small confidence interval, as standard error functions as the main component in the formula for normal distribution confidence intervals. Standard errors were calculated at each decile of the items up to 90% (0% smallest SE, 10% ..., 90% largest SE). The lower deciles would represent a strict criterion for accurate measurement, as many items would need smaller SEs to meet AIPE goals, while the higher deciles would represent less strict criterions for AIPE goals. 

```{r}
# calculate the SEs and the cutoff scores 
SES1 <- SES2 <- SES3 <- list()
cutoffs1 <- cutoffs2 <- cutoffs3 <- list()
for (i in 1:length(samples1)){
  SES1[[i]] <- tapply(samples1[[i]]$score,
                     list(samples1[[i]]$item,
                          samples1[[i]]$scale),
                     function (x){ sd(x)/sqrt(length(x))})
  SES2[[i]] <- tapply(samples2[[i]]$score,
                   list(samples2[[i]]$item,
                        samples2[[i]]$scale),
                   function (x){ sd(x)/sqrt(length(x))})
  SES3[[i]] <- tapply(samples3[[i]]$score,
                 list(samples2[[i]]$item,
                      samples2[[i]]$scale),
                 function (x){ sd(x)/sqrt(length(x))})

  cutoffs1[[i]] <- apply(as.data.frame(SES1[[i]]), 2, 
                         quantile, 
                         probs = seq(0, .9, by = .1))
  cutoffs2[[i]] <- apply(as.data.frame(SES2[[i]]), 2, 
                         quantile, 
                         probs = seq(0, .9, by = .1))
  cutoffs3[[i]] <- apply(as.data.frame(SES3[[i]]), 2, 
                         quantile, 
                         probs = seq(0, .9, by = .1))
}
```

## AIPE Simulation 

In this section, we simulate what a researcher might do if they follow our suggested application of AIPE to sample size planning based on well measured items. Assuming each pilot sample represents a dataset a researcher has collected, we will simulate samples of 20 to 500 to determine what the new sample size suggestion would be. We assume that samples over 500 may be considered too large for many researchers who do not work in teams or have participant funds. The standard error of each item was calculated for each suggested sample size by pilot sample size by population type.    

```{r}
# sequence of sample sizes to try
samplesize_values <- seq(20, 500, 5)

# place to store everything
sampled_values1 <- sampled_values2 <- sampled_values3 <- list()

# loop over the samples
for (i in 1:length(samples1)){
  
  # create a blank table for us to save the values in 
  sim_table1 <- matrix(NA, 
                      nrow = length(samplesize_values), 
                      ncol = 30*3)
  
  # make it a data frame
  sim_table1 <- sim_table2 <- sim_table3 <- as.data.frame(sim_table1)
  
  # add a place for sample size values 
  sim_table1$sample_size <- sim_table2$sample_size <- sim_table3$sample_size <- NA
  
  # loop over pilot sample sizes
  for (q in 1:length(samplesize_values)){
      
    # temp dataframe that samples and summarizes
    temp <- samples1[[i]] %>% 
      group_by(item, scale) %>% 
      slice_sample(n = samplesize_values[q], replace = T) %>% 
      summarize(se = sd(score)/sqrt(length(score)),
                .groups = "keep")
    
    sim_table1[q, 1:90] <- temp$se
    sim_table1[q, 91] <- samplesize_values[q]
    
    temp <- samples2[[i]] %>% 
      group_by(item, scale) %>% 
      slice_sample(n = samplesize_values[q], replace = T) %>% 
      summarize(se = sd(score)/sqrt(length(score)),
                .groups = "keep")
    
    sim_table2[q, 1:90] <- temp$se
    sim_table2[q, 91] <- samplesize_values[q]
    
    temp <- samples3[[i]] %>% 
      group_by(item, scale) %>% 
      slice_sample(n = samplesize_values[q], replace = T) %>% 
      summarize(se = sd(score)/sqrt(length(score)),
                .groups = "keep")
    
    sim_table3[q, 1:90] <- temp$se
    sim_table3[q, 91] <- samplesize_values[q]
    
    } # end pilot sample loop 
  
  sampled_values1[[i]] <- sim_table1
  sampled_values2[[i]] <- sim_table2
  sampled_values3[[i]] <- sim_table3

} # end all sample loop 
```

Next, the percent of items that fall below the cutoff scores, and thus, would be considered "well-measured" were calculated for each decile by sample. 

```{r}
summary_list1 <- summary_list2 <- summary_list3 <- list()
for (i in 1:length(sampled_values1)){
  
  # summary list 1 ----
  summary_list1[[i]] <- sampled_values1[[i]] %>% 
    pivot_longer(cols = -c(sample_size)) %>% 
    rename(item = name, se = value) %>% 
    mutate(scale = rep(1:3, 30*length(samplesize_values))) %>% 
    mutate(item = rep(rep(1:30, each = 3), length(samplesize_values))) 
    
  # cut offs for 1
  temp1.1 <- summary_list1[[i]] %>% 
    filter(scale == "1") %>% 
    group_by(sample_size, scale) %>% 
    summarize(Percent_Below0 = sum(se <= cutoffs1[[i]][1, 1])/30,
           Percent_Below10 = sum(se <= cutoffs1[[i]][2, 1])/30,
           Percent_Below20 = sum(se <= cutoffs1[[i]][3, 1])/30,
           Percent_Below30 = sum(se <= cutoffs1[[i]][4, 1])/30,
           Percent_Below40 = sum(se <= cutoffs1[[i]][5, 1])/30,
           Percent_Below50 = sum(se <= cutoffs1[[i]][6, 1])/30, 
           Percent_Below60 = sum(se <= cutoffs1[[i]][7, 1])/30, 
           Percent_Below70 = sum(se <= cutoffs1[[i]][8, 1])/30, 
           Percent_Below80 = sum(se <= cutoffs1[[i]][9, 1])/30, 
           Percent_Below90 = sum(se <= cutoffs1[[i]][10, 1])/30, 
           .groups = "keep") %>% 
    mutate(original_n = sizes[i], 
           source = "low")
    
  temp1.2 <- summary_list1[[i]] %>% 
    filter(scale == "2") %>% 
    group_by(sample_size, scale) %>% 
    summarize(Percent_Below0 = sum(se <= cutoffs1[[i]][1, 2])/30,
           Percent_Below10 = sum(se <= cutoffs1[[i]][2, 2])/30,
           Percent_Below20 = sum(se <= cutoffs1[[i]][3, 2])/30,
           Percent_Below30 = sum(se <= cutoffs1[[i]][4, 2])/30,
           Percent_Below40 = sum(se <= cutoffs1[[i]][5, 2])/30,
           Percent_Below50 = sum(se <= cutoffs1[[i]][6, 2])/30, 
           Percent_Below60 = sum(se <= cutoffs1[[i]][7, 2])/30, 
           Percent_Below70 = sum(se <= cutoffs1[[i]][8, 2])/30, 
           Percent_Below80 = sum(se <= cutoffs1[[i]][9, 2])/30, 
           Percent_Below90 = sum(se <= cutoffs1[[i]][10, 2])/30, 
           .groups = "keep") %>% 
    mutate(original_n = sizes[i],
           source = "low")
  
  temp1.3 <- summary_list1[[i]] %>% 
    filter(scale == "3") %>% 
    group_by(sample_size, scale) %>% 
    summarize(Percent_Below0 = sum(se <= cutoffs1[[i]][1, 3])/30,
           Percent_Below10 = sum(se <= cutoffs1[[i]][2, 3])/30,
           Percent_Below20 = sum(se <= cutoffs1[[i]][3, 3])/30,
           Percent_Below30 = sum(se <= cutoffs1[[i]][4, 3])/30,
           Percent_Below40 = sum(se <= cutoffs1[[i]][5, 3])/30,
           Percent_Below50 = sum(se <= cutoffs1[[i]][6, 3])/30, 
           Percent_Below60 = sum(se <= cutoffs1[[i]][7, 3])/30, 
           Percent_Below70 = sum(se <= cutoffs1[[i]][8, 3])/30, 
           Percent_Below80 = sum(se <= cutoffs1[[i]][9, 3])/30, 
           Percent_Below90 = sum(se <= cutoffs1[[i]][10, 3])/30, 
           .groups = "keep") %>% 
    mutate(original_n = sizes[i], 
           source = "low")
  
  #rejoin 
  summary_list1[[i]] <- bind_rows(temp1.1, temp1.2, temp1.3)
  
  # summary list 2 ----
  summary_list2[[i]] <- sampled_values2[[i]] %>% 
    pivot_longer(cols = -c(sample_size)) %>% 
    rename(item = name, se = value) %>% 
    mutate(scale = rep(1:3, 30*length(samplesize_values))) %>% 
    mutate(item = rep(rep(1:30, each = 3), length(samplesize_values)))
  
  # cut offs for 2
  temp2.1 <- summary_list2[[i]] %>% 
    filter(scale == "1") %>% 
    group_by(sample_size, scale) %>% 
    summarize(Percent_Below0 = sum(se <= cutoffs2[[i]][1, 1])/30,
           Percent_Below10 = sum(se <= cutoffs2[[i]][2, 1])/30,
           Percent_Below20 = sum(se <= cutoffs2[[i]][3, 1])/30,
           Percent_Below30 = sum(se <= cutoffs2[[i]][4, 1])/30,
           Percent_Below40 = sum(se <= cutoffs2[[i]][5, 1])/30,
           Percent_Below50 = sum(se <= cutoffs2[[i]][6, 1])/30, 
           Percent_Below60 = sum(se <= cutoffs2[[i]][7, 1])/30, 
           Percent_Below70 = sum(se <= cutoffs2[[i]][8, 1])/30, 
           Percent_Below80 = sum(se <= cutoffs2[[i]][9, 1])/30, 
           Percent_Below90 = sum(se <= cutoffs2[[i]][10, 1])/30, 
           .groups = "keep") %>% 
    mutate(original_n = sizes[i],
           source = "med")
    
  temp2.2 <- summary_list2[[i]] %>% 
    filter(scale == "2") %>% 
    group_by(sample_size, scale) %>% 
    summarize(Percent_Below0 = sum(se <= cutoffs2[[i]][1, 2])/30,
           Percent_Below10 = sum(se <= cutoffs2[[i]][2, 2])/30,
           Percent_Below20 = sum(se <= cutoffs2[[i]][3, 2])/30,
           Percent_Below30 = sum(se <= cutoffs2[[i]][4, 2])/30,
           Percent_Below40 = sum(se <= cutoffs2[[i]][5, 2])/30,
           Percent_Below50 = sum(se <= cutoffs2[[i]][6, 2])/30, 
           Percent_Below60 = sum(se <= cutoffs2[[i]][7, 2])/30, 
           Percent_Below70 = sum(se <= cutoffs2[[i]][8, 2])/30, 
           Percent_Below80 = sum(se <= cutoffs2[[i]][9, 2])/30, 
           Percent_Below90 = sum(se <= cutoffs2[[i]][10, 2])/30, 
           .groups = "keep") %>% 
    mutate(original_n = sizes[i], 
           source = "med")
  
  temp2.3 <- summary_list2[[i]] %>% 
    filter(scale == "3") %>% 
    group_by(sample_size, scale) %>% 
    summarize(Percent_Below0 = sum(se <= cutoffs2[[i]][1, 3])/30,
           Percent_Below10 = sum(se <= cutoffs2[[i]][2, 3])/30,
           Percent_Below20 = sum(se <= cutoffs2[[i]][3, 3])/30,
           Percent_Below30 = sum(se <= cutoffs2[[i]][4, 3])/30,
           Percent_Below40 = sum(se <= cutoffs2[[i]][5, 3])/30,
           Percent_Below50 = sum(se <= cutoffs2[[i]][6, 3])/30, 
           Percent_Below60 = sum(se <= cutoffs2[[i]][7, 3])/30, 
           Percent_Below70 = sum(se <= cutoffs2[[i]][8, 3])/30, 
           Percent_Below80 = sum(se <= cutoffs2[[i]][9, 3])/30, 
           Percent_Below90 = sum(se <= cutoffs2[[i]][10, 3])/30, 
           .groups = "keep") %>% 
    mutate(original_n = sizes[i],
           source = "med")
  
  #rejoin 
  summary_list2[[i]] <- bind_rows(temp2.1, temp2.2, temp2.3)
  
  # summary list 3 ----
  summary_list3[[i]] <- sampled_values3[[i]] %>% 
    pivot_longer(cols = -c(sample_size)) %>% 
    rename(item = name, se = value) %>% 
    mutate(scale = rep(1:3, 30*length(samplesize_values))) %>% 
    mutate(item = rep(rep(1:30, each = 3), length(samplesize_values)))
  
  # cut offs for 3 
  temp3.1 <- summary_list3[[i]] %>% 
    filter(scale == "1") %>% 
    group_by(sample_size, scale) %>% 
    summarize(Percent_Below0 = sum(se <= cutoffs3[[i]][1, 1])/30,
           Percent_Below10 = sum(se <= cutoffs3[[i]][2, 1])/30,
           Percent_Below20 = sum(se <= cutoffs3[[i]][3, 1])/30,
           Percent_Below30 = sum(se <= cutoffs3[[i]][4, 1])/30,
           Percent_Below40 = sum(se <= cutoffs3[[i]][5, 1])/30,
           Percent_Below50 = sum(se <= cutoffs3[[i]][6, 1])/30, 
           Percent_Below60 = sum(se <= cutoffs3[[i]][7, 1])/30, 
           Percent_Below70 = sum(se <= cutoffs3[[i]][8, 1])/30, 
           Percent_Below80 = sum(se <= cutoffs3[[i]][9, 1])/30, 
           Percent_Below90 = sum(se <= cutoffs3[[i]][10, 1])/30, 
           .groups = "keep") %>% 
    mutate(original_n = sizes[i],
           source = "high")
     
  temp3.2 <- summary_list3[[i]] %>% 
    filter(scale == "2") %>% 
    group_by(sample_size, scale) %>% 
    summarize(Percent_Below0 = sum(se <= cutoffs3[[i]][1, 2])/30,
           Percent_Below10 = sum(se <= cutoffs3[[i]][2, 2])/30,
           Percent_Below20 = sum(se <= cutoffs3[[i]][3, 2])/30,
           Percent_Below30 = sum(se <= cutoffs3[[i]][4, 2])/30,
           Percent_Below40 = sum(se <= cutoffs3[[i]][5, 2])/30,
           Percent_Below50 = sum(se <= cutoffs3[[i]][6, 2])/30, 
           Percent_Below60 = sum(se <= cutoffs3[[i]][7, 2])/30, 
           Percent_Below70 = sum(se <= cutoffs3[[i]][8, 2])/30, 
           Percent_Below80 = sum(se <= cutoffs3[[i]][9, 2])/30, 
           Percent_Below90 = sum(se <= cutoffs3[[i]][10, 2])/30, 
           .groups = "keep") %>% 
    mutate(original_n = sizes[i],
           source = "high")
  
  temp3.3 <- summary_list3[[i]] %>% 
    filter(scale == "3") %>% 
    group_by(sample_size, scale) %>% 
    summarize(Percent_Below0 = sum(se <= cutoffs3[[i]][1, 3])/30,
           Percent_Below10 = sum(se <= cutoffs3[[i]][2, 3])/30,
           Percent_Below20 = sum(se <= cutoffs3[[i]][3, 3])/30,
           Percent_Below30 = sum(se <= cutoffs3[[i]][4, 3])/30,
           Percent_Below40 = sum(se <= cutoffs3[[i]][5, 3])/30,
           Percent_Below50 = sum(se <= cutoffs3[[i]][6, 3])/30, 
           Percent_Below60 = sum(se <= cutoffs3[[i]][7, 3])/30, 
           Percent_Below70 = sum(se <= cutoffs3[[i]][8, 3])/30, 
           Percent_Below80 = sum(se <= cutoffs3[[i]][9, 3])/30, 
           Percent_Below90 = sum(se <= cutoffs3[[i]][10, 3])/30, 
           .groups = "keep") %>% 
    mutate(original_n = sizes[i],
           source = "high")
  
  #rejoin 
  summary_list3[[i]] <- bind_rows(temp3.1, temp3.2, temp3.3)

  }

summary_DF <- bind_rows(summary_list1, 
                        summary_list2, 
                        summary_list3)
```

From this data, we pinpoint the smallest suggested sample size at which 80%, 85%, 90%, and 95% of the items fall below the cutoff criterion. These values were chosen as popular measures of "power" in which one could determine the minimum suggested sample size (potentially 80% of items) and the maximum suggested sample size (potentially 90%). 

```{r}
summary_long_80 <- summary_DF %>% 
  pivot_longer(cols = -c(sample_size, original_n, source, scale)) %>% 
  filter(value >= .80) %>% 
  arrange(sample_size, original_n, source, scale, name) %>% 
  group_by(original_n, name, source, scale) %>% 
  slice_head(n = 1) %>% 
  mutate(power = 80)

summary_long_85 <- summary_DF %>% 
  pivot_longer(cols = -c(sample_size, original_n, source, scale)) %>% 
  filter(value >= .85) %>% 
  arrange(sample_size, original_n, source, scale, name) %>% 
  group_by(original_n, name, source, scale) %>% 
  slice_head(n = 1) %>% 
  mutate(power = 85)
  
summary_long_90 <- summary_DF %>% 
  pivot_longer(cols = -c(sample_size, original_n, source, scale)) %>% 
  filter(value >= .90) %>% 
  arrange(sample_size, original_n, source, scale, name) %>% 
  group_by(original_n, name, source, scale) %>% 
  slice_head(n = 1) %>% 
  mutate(power = 90)

summary_long_95 <- summary_DF %>% 
  pivot_longer(cols = -c(sample_size, original_n, source, scale)) %>% 
  filter(value >= .95) %>% 
  arrange(sample_size, original_n, source, scale, name) %>% 
  group_by(original_n, name, source, scale) %>% 
  slice_head(n = 1) %>% 
  mutate(power = 95)

summary_long <- rbind(summary_long_80, 
                      summary_long_85,
                      summary_long_90,
                      summary_long_95)

summary_long$source <- factor(summary_long$source, 
                              levels = c("low", "med", "high"),
                              labels = c("Low", "Medium", "High"))

summary_long$sd_original <- .2
summary_long$sd_original[summary_long$source == "Medium" &
                           summary_long$scale == "1"] <- .4
summary_long$sd_original[summary_long$source == "High" &
                           summary_long$source == "1"] <- .8

summary_long$sd_original[summary_long$source == "Low" &
                           summary_long$scale == "2"] <- 4
summary_long$sd_original[summary_long$source == "Medium" &
                           summary_long$scale == "2"] <- 8
summary_long$sd_original[summary_long$source == "High" &
                           summary_long$source == "2"] <- 12

summary_long$sd_original[summary_long$source == "Low" &
                           summary_long$scale == "3"] <- 100
summary_long$sd_original[summary_long$source == "Medium" &
                           summary_long$scale == "3"] <- 150
summary_long$sd_original[summary_long$source == "High" &
                           summary_long$source == "3"] <- 200


summary_long$name <- gsub("Percent_Below", "Decile ", summary_long$name)
```

# Results

## Differences in Item Variance

We examined if this procedure is sensitive to differences in item heterogeneity, as we should expect to collect larger samples if we wish to have a large number of items reach a threshold of acceptable variance; potentially, assuring we *could* average them if a researcher did not wish to use a more complex analysis such as multilevel modeling. 

The figure below illustrates the potential minimum sample size for 80% of items to achieve a desired cutoff score. The black dots denote the original sample size against the suggested sample size. By comparing the facets, we can determine that our suggested procedure does capture the differences in heterogeneity. As heterogeneity increases in item variances, the proposed sample size also increases, especially at stricter cutoffs. Missing cutoff points where sample sizes proposed would be higher than 500. 

```{r}
ggplot(summary_long %>% filter(power == 80), aes(original_n, sample_size, color = name)) + 
  geom_point() +  
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  theme_classic() + 
  facet_wrap(~ source) + 
  xlab("Original Sample Size") + 
  ylab("Suggested Sample Size") + 
  scale_color_discrete(name = "Cutoff Score")
```

## Sample Size Sensitivity to Pilot Data Size

In our second question, we examined if the suggested procedure was sensitive to the amount of information present in the pilot data. Larger pilot data is more informative, and therefore, we should expect a lower projected sample size. As shown in the figure below for only the low variability data, we do not find this effect. These simplistic simulations from the pilot data would nearly always suggest a larger sample size - mostly in a linear trend increasing with sample sizes. This result comes from the nature of the procedure - if we base our estimates on some SE cutoff, we will almost always need a bit more people for items to meet those goals. This result does not achieve our second goal. 

```{r}
ggplot(summary_long %>% filter(source == "Low"), aes(original_n, sample_size, color = name)) + 
  geom_point() +  
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  theme_classic() + 
  facet_wrap(~ power) + 
  xlab("Original Sample Size") + 
  ylab("Suggested Sample Size") + 
  scale_color_discrete(name = "Cutoff Score")
```

Therefore, we suggest using a correction factor on the simulation procedure to account for the known asymptotic nature of power (i.e., at larger sample sizes power increases level off). For this function, we combined a correction factor for upward biasing of effect sizes (Hedges' correction) with the formula for exponential decay calculations. The decay factor is calculated as follows: 

$$ 1 - \sqrt{\frac{N_{pilot} - min(N_{sim})}{N_{pilot}}}^{log_2(N_{pilot})}$$
$N_{pilot}$ indicates the sample size of the pilot data minus the minimum sample size for simulation to ensure that the smallest sample sizes do not decay (e.g., the formula zeroes out). This value is raised to the power of $log_2$ of the sample size of the pilot data, which decreases the impact of the decay to smaller increments for increasing sample sizes. This value is then multiplied by the proposed sample size. As show in the figure below, this correction factor produces the dsired quality of maintaining that small pilot studies should *increase* sample size, and that sample size suggestions level off as pilot study data sample size increases. 

```{r}
decay <- 1-sqrt((summary_long$original_n-20)/summary_long$original_n)^log2(summary_long$original_n)

summary_long$new_sample <- summary_long$sample_size*decay

ggplot(summary_long %>% filter(source == "Low"), aes(original_n, new_sample, color = name)) + 
  geom_point() +  
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  theme_classic() + 
  facet_wrap(~ power) + 
  xlab("Original Sample Size") + 
  ylab("Suggested Sample Size") + 
  scale_color_discrete(name = "Cutoff Score")
```

## Corrections for Individual Researchers

We have portrayed that this procedure, with a correction factor, can perform as desired. However, within real scenarios, researchers will only have one pilot sample, not many as show here. What should the researcher do to correct their sample size on their own pilot data? 

To explore if we could recover the new suggested sample size from data a researcher would have, we used linear models to create a formula for calculation. First, the corrected sample size was predicted by the original suggested sample size. Next, the standard deviation of the item standard deviations was added to the equation. Last, we included the pilot sample size. 

```{r}
user_model <- lm(new_sample ~ sample_size, data = summary_long)
user_print <- apa_print(user_model)

user_model2 <- lm(new_sample ~ sample_size + sd_original, data = summary_long)
user_print2 <- apa_print(user_model2)

user_model3 <- lm(new_sample ~ sample_size + sd_original + original_n, data = summary_long)
user_print3 <- apa_print(user_model3)

user_model4 <- lm(new_sample ~ sample_size + sd_original + original_n + scale, data = summary_long)
user_print4 <- apa_print(user_model4)

change_table <- tidy(anova(user_model, user_model2, user_model3))
```

The first model using original sample size to predict new sample size was significant, `r user_print$full_result$modelfit$r2`, capturing nearly 90% of the variance. The second model with item standard deviation was better then the first model *F*(`r change_table$df[2]`, `r change_table$res.df[2]`) = `r change_table$statistic[2]`, *p* < .001, `r user_print2$estimate$modelfit$r2`. The addition of the original pilot sample size was also significant, *F*(`r change_table$df[3]`, `r change_table$res.df[3]`) = `r change_table$statistic[3]`, *p* < .001, `r user_print3$estimate$modelfit$r2`.

As shown in the table below, the new suggested sample size is proportional to the original suggested sample size (i.e., *b* < 1), which reduces the sample size suggestion. As variability increases, the suggested sample size also increases to capture differences in heterogeneity shown above. Last, in order to correct for large pilot data, the original pilot sample size decreases the new suggested sample size. This formula approximation captures 96% of the variance in sample size scores and should allow a researcher to estimate based on their own data. 

```{r results = 'asis'}
apa_table(tidy(user_model3))
```

## Choosing an Appropriate Cutoff

```{r}
by_cutoff <- list()
R2 <- list()

for (cutoff in unique(summary_long$name)){
  by_cutoff[[cutoff]] <- lm(new_sample ~ sample_size + sd_original + original_n, data = summary_long  %>% filter(name == cutoff))
  R2[cutoff] <- summary(by_cutoff[[cutoff]])$r.squared
}

R2
```

Last, we examine the question of an appropriate SE decile. All graphs for power, variability, and correction are presented below. If we examine the $R^2$ values for each decile of our regression equation separately, we find that the 10% (`r round(R2[["Decile 10"]], digits = 3)`) and 30% deciles (`r round(R2[["Decile 30"]], digits = 3)`) represent the best match to our corrected sample size suggestions. The 10% is likely unfeasible for many researchers, as the sample sizes are often quite large. The 30% decile, in the corrected format, appears to meet all goals: 1) increases with heterogeneity, 2) higher suggested values for lower samples and a leveling effect at larger pilot data, and ...

### Low Variability 

```{r}
ggplot(summary_long %>% filter(source == "Low"), aes(original_n, sample_size, color = name)) + 
  geom_point() +  
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  theme_classic() + 
  facet_wrap(~ power) + 
  xlab("Original Sample Size") + 
  ylab("Suggested Sample Size") + 
  scale_color_discrete(name = "Cutoff Score")
```

```{r}
ggplot(summary_long %>% filter(source == "Low"), aes(original_n, new_sample, color = name)) + 
  geom_point() +  
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  theme_classic() + 
  facet_wrap(~ power) + 
  xlab("Original Sample Size") + 
  ylab("Suggested Sample Size") + 
  scale_color_discrete(name = "Cutoff Score")
```

### Medium Varability

```{r}
ggplot(summary_long %>% filter(source == "Medium"), aes(original_n, sample_size, color = name)) + 
  geom_point() +  
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  theme_classic() + 
  facet_wrap(~ power) + 
  xlab("Original Sample Size") + 
  ylab("Suggested Sample Size") + 
  scale_color_discrete(name = "Cutoff Score")
```

```{r}
summary_long$new_sample <- summary_long$sample_size*decay

ggplot(summary_long %>% filter(source == "Medium"), aes(original_n, new_sample, color = name)) + 
  geom_point() +  
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  theme_classic() + 
  facet_wrap(~ power) + 
  xlab("Original Sample Size") + 
  ylab("Suggested Sample Size") + 
  scale_color_discrete(name = "Cutoff Score")
```

### High Variability

```{r}
ggplot(summary_long %>% filter(source == "High"), aes(original_n, sample_size, color = name)) + 
  geom_point() +  
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  theme_classic() + 
  facet_wrap(~ power) + 
  xlab("Original Sample Size") + 
  ylab("Suggested Sample Size") + 
  scale_color_discrete(name = "Cutoff Score")
```

```{r}
ggplot(summary_long %>% filter(source == "High"), aes(original_n, new_sample, color = name)) + 
  geom_point() +  
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  theme_classic() + 
  facet_wrap(~ power) + 
  xlab("Original Sample Size") + 
  ylab("Suggested Sample Size") + 
  scale_color_discrete(name = "Cutoff Score")
```












