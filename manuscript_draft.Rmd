---
title       : "Accuracy in Parameter Estimation and Simulation Approaches for Sample Size Planning with Multiple Stimuli"
shorttitle    : "AIPE BOOTSTRAPPING"
author: 
- name     : "Erin M. Buchanan"
  affiliation  : "1"
  corresponding : yes  # Define only one corresponding author
  address    : "326 Market St., Harrisburg, PA, 17101"
  email     : "ebuchanan@harrisburgu.edu"
  role:     # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
   - Conceptualization
   - Writing - Original Draft Preparation
   - Writing - Review & Editing
   - Resources
   - Validation 
   - Visualization 
   - Project Administration 
   - Formal Analysis
- name     : "Other Folks as Per Order on Doc"
  affiliation  : "2"
  role:
   - Writing - Review & Editing
   - Data Curation
   - Resources
affiliation:
- id      : "1"
  institution  : "Harrisburg University of Science and Technology"
- id      : "2"
  institution  : "Other Instituions"
authornote: |
  
abstract: |
 The planning of the sample size for research studies often focuses on obtaining a significant result given a specified level of power, significance, and an anticipated effect size. This planning requires prior knowledge of the study design and a statistical analysis to calculate the proposed sample size. However, there may not be one specific testable analysis from which to derive power [@silberzahn2018many] or a hypothesis to test for the project (e.g., creation of a stimuli database). Modern power and sample size planning suggestions include accuracy in parameter estimation [AIPE, @kelley2007; @maxwell2008] and simulation of proposed analyses [@chalmers2020]. These toolkits provide flexibility in traditional power analyses that focus on the if-this, then-that approach, yet, both AIPE and simulation require either a specific parameter (e.g., mean, effect size, etc.) or statistical test for planning sample size. In this tutorial, we explore how AIPE and simulation approaches can be combined to accommodate studies that may not have a specific hypothesis test or wish to account for the potential of a multiverse of analyses. Specifically, the examples focus on studies that adopt multiple items and suggest that sample sizes can be planned to measure those items adequately and precisely, regardless of statistical test. We demonstrate that pilot data can be used to determine a sample size that represents well-measured data. This tutorial also provides multiple code vignettes that researchers can adapt and apply to their own measures.
 
keywords     : "accuracy in parameter estimation, power, sampling, simulation, hypothesis testing"
floatsintext   : no
figurelist    : no
tablelist     : no
footnotelist   : no
linenumbers    : yes
mask       : no
draft       : no
classoption    : "man"
bibliography: references.bib
header-includes:
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE, echo = FALSE}
set.seed(895893)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
library(dplyr)
library(ggplot2)
library(tidyr)
library(broom)
library(papaja)
library(rio)
library(nlme)
library(MuMIn)
library(pwr)
library(latex2exp)
```

```{r trackdown, eval = F}
# devtools::install_github("ClaudioZandonella/trackdown")
library(trackdown)
# initial set up 
# upload_file(file = "manuscript_draft.Rmd",
#       gfile = "power2stim_V2_trackdown",
#       gpath = "Studies/PSA007 SPAM-L/Manuscripts/Power",
#       shared_drive = "Psychological Science Accelerator")
# from google 
download_file(file = "manuscript_draft.Rmd", 
      gfile = "power2stim_V2_trackdown",
      gpath = "Studies/PSA007 SPAM-L/Manuscripts/Power",
      shared_drive = "Psychological Science Accelerator")
# to google
update_file(file = "manuscript_draft.Rmd", 
      gfile = "power2stim_V2_trackdown",
      gpath = "Studies/PSA007 SPAM-L/Manuscripts/Power",
      shared_drive = "Psychological Science Accelerator",
      rich_text = TRUE,
      hide_code = TRUE)
```

An inevitable decision in almost any empirical research is deciding on the sample size. Statistical power and power analyses are arguably some of the most important components in planning a research study and its corresponding sample size [@cohen1990]. However, if reviews of transparency and openness in research publications are any clue, researchers in the social sciences commonly fail to implement proper power analyses as part of their research workflow [@hardwicke2020; @doi:10.1177/1745691620979806]. The replication "crisis" and credibility revolution have shown that published studies in psychology are underpowered [@opensciencecollaboration2015; @vazire2018, CITE KORBMACHER]. Pre-registration of a study involves outlining the study and hypotheses before data collection begins [@stewart2020; @d.chambers2014; @nosek2014], and details of a power analyses or limitations on resources are often used to provide justification for the pre-registered sample quota [CITE (Pownall et al., 2023; van den Akker et al., 2023a, 2023b)]. Given the combined issues of publish-or-perish and that most non-significant results do not result in published manuscripts, power analysis may be especially critical for early career researchers to increase the likelihood that they will identify significant effects if they exist [@rosenthal1979; @simmons2011]. Justified sample sizes may allow for publication of non-significant, yet practically important effects, along with the smallest effect of interest movement [CITE DANIEL/ANVARI], potentially improving the credibility of published work. At best, an underpowered study provides limited insight [@halpern2002], and it can be difficult to know if a poorly implemented power analysis is better than no power analysis.

A recent review of power analyses found - across behavioral, cognitive, and social science journal articles - researchers did not provide enough information to understand their power analyses and often chose effect sizes that were unjustified [@beribisky2019]. One solution to this power analysis problem is the plethora of tools made available for researchers to make power computations accessible to non-statisticians; however, a solid education in power is necessary to use these tools properly. G\*Power is one of the most popular free power software options [@faul2007; @erdfelder1996] that provides a simple point and click graphical user interface for power calculations [however, see @brysbaert2019]. Web-based tools have also sprung up for overall and statistical test specific sample size planning including <https://powerandsamplesize.com>, <https://jakewestfall.shinyapps.io/pangea/> , <https://pwrss.shinyapps.io/index/>, and <https://designingexperiments.com> [@anderson2017]. *R*-coding based packages, such as *pwr* [@champely2017], *faux* [@debruine2021], *simr* [CITE GREEN], *mixedpower* [CITE KUMLE], and *SimDesign* [@chalmers2020], can be used to examine power and plan sample sizes, usually with simulation. Researchers must be careful using any toolkit, as errors can occur with the over-reliance on software [e.g., it should not be a substitute for critical thinking, @nuijten2016]. Additionally, many tools assume data normality, place an overemphasis on statistical significance, and may rely on simplified assumptions that do not reflect the actual data. When computing sample size estimates, it is important to remember that these values are estimations, not exact calculations guaranteed to produce a specific result [@batterham2005]. For example, it is hard to accurately estimate all parameters from a study, and if any were incorrect, then the sample size estimate tied to that specific level of power may be incorrect [CITE ALBERS]. 

Power analyses are not always the best tool for the job. Changes in publication practices and research design have also created new challenges in providing a sample size plan for a research study. While statistics courses often suggest that a specific research design leads to a specific statistical test, meta-science work has shown that given the same data and hypothesis, researchers can come up with multiple ways to analyze the data [@silberzahn2018many, @coretta]. Therefore, a single power analysis only corresponds to the specific analysis that the researcher expects to implement. Analyses may evolve during the research project or be subject to secondary analysis; thus, power and sample size estimation based on one analysis is potentially less useful than previously imagined. Further, research projects often have multiple testable hypotheses, but it is unclear which hypothesis or test should be used to estimate sample size with a power analysis. Last, research investigations may not even have a specific, testable hypothesis, as some projects are intended to curate a large dataset for future reuse [i.e., stimuli database creation, @buchanan2019].

In light of these analytical (or lack thereof) concerns, we propose a new method to determine a sample size in cases where a more traditional power analysis might be less appropriate or even impossible. This approach combines accuracy in parameter estimation [AIPE, @kelley2007, @maxwell2008] and bootstrapped simulation on pilot data [@rousselet]. This method accounts for a potential lack of hypothesis test (or simply no good way to estimate an effect size of interest), and/or an exploratory design with an unknown set of potential hypotheses and analytical choices. Specifically this manuscript focuses on research designs that use multiple items to measure the phenomena of interest. For example, semantic priming is measured with multiple paired stimuli [@meyer1971], which traditionally has been analyzed by creating person or item-level averages to test using an ANOVA [@brysbaert2018]. However, research implementing multilevel models with random effects for the stimuli used in a study has demonstrated potential variability in their impact on outcomes; thus we should be careful not to assume that all items in a research study have the same "effect".

## Accuracy in Parameter Estimation

AIPE shifts the focus away from finding a significant p-value to finding a parameter that is "accurately measured". For example, researchers may wish to detect a specific mean in a study, *M* = .35. They could then use AIPE to estimate the sample size needed to find a "sufficiently narrow" confidence interval around that mean. Sufficiently narrow is often defined by the researcher using a minimum parameter size of interest and confidence intervals. Therefore, they could decide that their 95% confidence interval should be approximately between .20 and .50, and sufficiently narrow could be defined as a width of .30 or .15 on each side. While confidence intervals are related to null hypothesis significance testing (i.e., 95% confidence intervals that do not include zero would indicate a significant difference from zero at $\alpha$ \< .05), AIPE procedures suggest how we can define a sample size with a given width of confidence interval, regardless of whether it includes zero.

## Bootstrapping and Simulation

One form of data simulation is Bootstrapping, which involves using data obtained to simulate similar datasets by drawing from the original data with replacement [@efron2000; @rousselet]. Bootstrapping allows one to calculate parameter estimates, confidence intervals, and to simulate the potential population distribution, shape, and bias. Simulation is often paired with re-creating a data set with a similar structure for testing analyses and hypotheses based on proposed effect sizes or suggested population means. Generally, we would suggest starting with pilot data of a smaller sample size (e.g., 20 to 50) to understand the variability in potential items used to represent your phenomenon, especially if they are to be used in a larger study. However, given some background knowledge about the potential items, one could simulate example pilot data to use in a similar manner in our suggested procedure. Pilot or simulated data would be used to estimate the variability within items and select a "sufficiently narrow" window for overall item confidence interval for AIPE (i.e., by selecting a specific standard error criterion, given the formula for confidence intervals). The advantage to this method over simple power estimation from pilot effect sizes is the multiple simulations to average out potential variability, as well as a shift away from traditional NHST to parameter estimation. Bootstrapping would then be used to determine how many participants may be necessary to achieve a dataset wherein as many items as required meet the pre-specified confidence interval. 

```{r echo = F, eval = F}
library(MBESS)
ss.aipe.R2(Population.R2 = 0.35^2, # our correlation squared
      conf.level = 0.95, # confidence interval 
      width = 0.30^2, # the width squared
      p = 1) # one predictor X to Y
```

## Sequential Testing

Researchers could then use sequential testing to estimate their parameter of interest after each participant's data or at regular intervals during data collection to determine whether they have achieved their expected width of the confidence interval around that parameter. One would set a minimum sample size (e.g.,, based on known data collection ability) and use the confidence interval width as a stopping rule (i.e., stop data collection when the CI is sufficiently narrow, as defined above). Next, researchers would use the estimated sample size associated with the simulation results of many items obtaining the stopping rule as a maximum sample size (e.g., , they expect 90% of items to meet their stopping rule with 100 participants based on simulation). By defining each of these components, researchers could ensure a feasible minimum sample size, a way to stop data collection when goals have been met, and a maximum sample size rule to ensure an actual end to data collection. The maximum stopping rule could also be defined by resources (e.g., two semesters data collection), but nevertheless should be included. Therefore, we propose a method that leverages the ideas behind AIPE, paired with simulation and bootstrapping, to estimate the minimum and maximum proposed sample sizes and stopping rules for studies that use multiple items with expected variability in their estimates to measure an overall phenomena. 

## Proposed Method for Sample Size Planning

Building on these ideas, we suggest the following procedure to determine a sample size for each item:

### Calculate the Stopping Rule

1) Use pilot data that closely resembles data you intend to collect. This dataset should contain items that are identical or similar to those that will be implemented in the study. In this procedure, it is important to ensure that the data is representative of a larger population of sampled items that you intend to assess. Generally, pilot data sample sizes will be smaller than the overall intended project (e.g., 20 to 50), as the goal would be to determine how many participants would be necessary to reach a "stable" standard error.

2) For each item in the pilot data, calculate the standard error (SE). Select a cutoff SE that defines when items are considered "accurately measured". The simulations described in the Data Simulation section will explore what criterion should be used to determine the cutoff SE from the pilot data. 

### Bootstrap Samples  

3) Sample, with replacement, from your pilot data using sample sizes starting at a value that you consider the minimal sample size per item  and increase in small units up to a value that you consider the maximum sample size. We will demonstrate example maximum sample sizes based on the data simulation below; however, a practical maximum sample size may be determined by time (e.g., one semester data collection) or resources (e.g., 200 participants worth of funding). As for the minimal sample size, we suggest using 20 as a reasonable value  for simulation purposes. For each sample size simulation, calculate the standard error for each item. Use multiple simulations (e.g., n = 500 to 1000) to avoid issues with random sampling variability.

### Determine Minimum, Maximum Sample Size

4) Use the simulated SEs to determine the percentage of items that meet the cutoff score determined in Step 2. Each sample size from Step 3 will have multiple bootstrapped simulations, and therefore, create an average percentage score for each sample size for Step 5. 

5) Find the minimum sample size so that 80%, 85%, 90%, and 95% of the items meet the cutoff score and can be considered accurately measured. We recommend these scores to ensure that most items are accurately measured, in a similar vein to the common power-criterion suggestions. Each researcher can determine which of these is their minimum or maximum sample size (e.g., individuals can choose to use 80% as a minimum and 90% as a maximum or use values from Step 3 based on resources).

### Report Results

6) Report these values, and designate a minimum sample size, the cutoff/stopping rule criterion, and the maximum sample size. Each researcher should also report if they plan to use an adaptive design, which would stop data collection after meeting the cutoff criterion for each item.

These steps are summarized in Table \@ref(tab:table-summary). We will first demonstrate the ideas behind the steps using open data [@brysbaert2014; @balota2007]. This example will reveal a few areas of needed exploration for the steps. Next, we portray simulations for the proposed procedure and find solutions to streamline and improve the sample size estimation procedure. Table \@ref(tab:table-summary) shows the results of the simulations and solutions on the right hand side. Finally, we include additional resources for researchers to use to implement the estimation procedure.

```{r table-summary, results = 'asis'}
apa_table(data.frame(
  "Step Number" = c("1", "2" , "3", "4", "5", "6"),
  "Proposed Steps" = c(
    "Use representative pilot data.",
    "Calculate standard error of each of the items in the pilot data. Determine the appropriate SE for the stopping rule.",
    "Create bootstrapped samples of your pilot data starting with at least 20 participants up to a maximum number of participants.", 
    "Calculate the standard error of each of the items in the bootstrapped data. From these scores, calculate the percent of items below the cutoff score from Step 2.",
    "Determine the sample size at which 80%, 85%, 90%, 95% of items are below the cutoff score.",
    "Report all values. Designate one as the minimum sample size, the cutoff score as the stopping rule for adaptive designs, and the maximum sample size."),
    "Updated Steps" = c("Use representative pilot data.",
    "Calculate standard error of each of the items in the pilot data. Using the 40%, determine the cutoff and stopping rule for the standard error of the items.",
    "Create bootstrapped samples of your pilot data starting with at least 20 participants up to a maximum number of participants.", 
    "Calculate the standard error of each of the items in the bootstrapped data. From these scores, calculate the percent of items below the cutoff score from Step 2.",
    "Determine the sample size at which 80%, 85%, 90%, 95% of items are below the cutoff score. Use the correction formula to adjust your proposed sample size based on pilot data size, power, and percent variability.",
    "Report all values. Designate one as the minimum sample size, the cutoff score as the stopping rule for adaptive designs, and the maximum sample size.")
  ), 
  caption = "Proposed Procedure for Powering Studies with Multiple Items",
  col.names = c("Step", "Proposed Steps", "Updated Steps"),
  align = c("m{1cm}", rep("m{6cm}", 2)),
  font_size = "footnotesize"
)
```

# Example

```{r import-data-example, include = F, echo = F}
conDF <- import("./data/concreteness_trial.rda")
elpDF <- import("./data/ELPDecisionData.zip")

# grab only the overlap for this fake study
elpDF <- elpDF %>% 
  filter(Stimulus %in% conDF$Word) %>% 
  filter(Type == 1)

conDF <- conDF %>% 
  filter(Word %in% elpDF$Stimulus) 
```

In this section, we provide an example of the suggested procedure. The first dataset includes concreteness ratings from @brysbaert2014. Instructions given to participants denoted the difference between concrete (i.e., "refers to something that exists in reality") and abstract (i.e., "something you cannot experience directly through your senses or actions") terms. Participants were then asked to rate concreteness of terms using a 1 (*abstract*) to 5 (*concrete*) scale. This data represents a small scale dataset that could be used as pilot data for a study using concrete word ratings. The data is available at <https://osf.io/qpmf4/>.

The second dataset includes a large scale dataset with response latencies, the English Lexicon Project [ELP; @balota2007]. The ELP consists of lexical decision response latencies for written English words and pseudowords. In a lexical decision task, participants simply select "word" for real words (e.g., *dog*) and "nonword" for pseudowords (e.g., *wug*). The trial level data is available here: <https://elexicon.wustl.edu/>. Critically, in each of these datasets, the individual trial level data for each item is available to simulate and calculate standard errors on. Data that has been summarized could potentially be used, as long as the original standard deviations for each item were present. From the mean and standard deviation for each item, a simulated pilot dataset could be generated for estimating new sample sizes. All code to estimate sample sizes is provided on our OSF page, and this manuscript was created with a *papaja* [@aust2022] formatted Rmarkdown document.

For this example, imagine a researcher who wants to determine the differences in response latencies for abstract and concrete words. They will select *n* = 40 words from the rating data from @brysbaert2014 that are split evenly into abstract and concrete ends of the rating scale. In the experiment, each participant will be asked to rate the words for their concreteness, and then complete a lexical decision task with these words as the phenomenon of interest. Using both datasets and the procedure outlined above, we can determine the sample size necessary to ensure adequately measured concreteness ratings and response latencies.

```{r con-estimate, echo = F, include = F}
# sample the data of with 20 words in each category
average_item_score <- conDF %>% 
  filter(Rating != "N" & Rating != "n") %>% 
  mutate(Rating = as.numeric(Rating)) %>% 
  group_by(Word) %>% 
  summarize(avgrating = mean(Rating))

con_words <- c(
  average_item_score %>% 
    filter(avgrating <= 2) %>% 
    sample_n(size = 20) %>% 
    pull(Word), 
  average_item_score %>% 
    filter(avgrating >= 4) %>% 
    sample_n(size = 20) %>% 
    pull(Word)
)

# filter the data 
con_small <- conDF %>% filter(Word %in% con_words)

# drop levels so R isn't dumb
con_small$Word <- droplevels(con_small$Word)

# figure out what size the pilot data actually is for each rating 
pilot_size_c <- round(mean(tapply(con_small$Rating, con_small$Word, length)))

# figure out data loss
con_small$Rating[con_small$Rating == "n" | con_small$Rating == "N"] <- NA
con_small$Rating <- as.numeric(con_small$Rating)
data_loss_c <- con_small %>% 
 group_by(Word) %>% 
 summarize(percent_correct = sum(!is.na(Rating))/n(), .groups = "keep")
con_use <- con_small %>% filter(!is.na(Rating))

# cutoff score options
SE_c <- tapply(con_use$Rating, con_use$Word, function (x) { sd(x)/sqrt(length(x))})
cutoff_c <- quantile(SE_c, probs = seq(from = .1, to = .9, by = .1))
```

*Step 1*. The concreteness ratings data includes `r length(unique(conDF$Word))` concepts that were rated for their concreteness. We randomly selected *n* = 20 abstract words ($M_{Rating}$ \<= 2) and *n* = 20 concrete words ($M_{Rating}$ \>= 4). In the original study, not every participant rated every word, which created uneven sample sizes for each word. Further, participants were allowed to indicate they did not know a word, and those responses were set to missing data. In our sample of 40 words, the average pilot sample size was `r round(mean(tapply(con_small$Rating, con_small$Word, length)), digits = 2)` (*SD* = `r round(sd(tapply(con_small$Rating, con_small$Word, length)), digits = 2)`), and we will use `r pilot_size_c` as our pilot sample size for this example.

```{r elp-estimate, echo = F, include = F}
elp_small <- elpDF %>% filter(Stimulus %in% con_words)
# elp_small$Stimulus <- droplevels(elp_small$Stimulus) already a character

# figure out pilot size
pilot_size_e <- round(mean(tapply(elp_small$RT, elp_small$Stimulus, length)))

# figure out data loss
elp_small$RT[elp_small$Accuracy == "0"] <- NA
data_loss_e <- elp_small %>% 
 group_by(Stimulus, Type) %>% 
 summarize(percent_correct = sum(!is.na(RT))/n(), .groups = "keep")
elp_use <- elp_small %>% filter(!is.na(RT))

# cutoff score 
SE_e <- tapply(elp_use$RT, elp_use$Stimulus, function (x) { sd(x)/sqrt(length(x))})
cutoff_e <- quantile(SE_e, probs = seq(from = .1, to = .9, by = .1))
```

We first filtered the ELP data to the same real words as the concreteness subset selected above, and this data includes `r length(unique(elpDF$Stimulus))` real words. The average pilot sample size for this random sample was `r format(round(mean(tapply(elp_small$RT, elp_small$Stimulus, length)), digits = 2), nsmall = 2)` (*SD* = `r round(sd(tapply(elp_small$RT, elp_small$Stimulus, length)), digits = 2)`), and *n* = `r pilot_size_e` will be our pilot size for this example.

*Step 2*. Table \@ref(tab:table-example) demonstrates the cutoff scores for deciles of the standard errors for the items for the concreteness ratings and lexical decision response latencies. A researcher could potentially pick any of these cutoffs or other percentage options not shown here. We will use simulation to determine the suggestion that best captures the balance of adequately powering our sample and feasibility.

*Step 3-5*. The pilot data was then simulated 500 times, with replacement, with samples of 20 to 300 participants per item increasing in units of 5, for concreteness ratings and lexical decision latencies separately (Step 3). The standard error of each item was calculated for the bootstrapped samples separately for concreteness ratings and lexical decision times (Step 4), and the percentage of items below each potential cutoff was gathered for each (Step 5). The smallest sample size with at least 80%, 85%, 90%, and 95% of items below the cutoff are reported in Table \@ref(tab:table-example) for each task (Step 5).

```{r con-sample, include = F, echo = F}
# sequence of sample sizes to try
nsim <- 5
samplesize_values <- seq(20, 300, 5)
# create a blank table for us to save the values in 
sim_table <- matrix(NA, 
          nrow = length(samplesize_values)*nsim, 
          ncol = length(unique(con_use$Word)))
# make it a data frame
sim_table <- as.data.frame(sim_table)
# add a place for sample size values 
sim_table$sample_size <- NA

iterate <- 1
for (p in 1:nsim){
  # loop over sample sizes
  for (i in 1:length(samplesize_values)){
    
   # temp dataframe that samples and summarizes
   temp <- con_use %>% 
    group_by(Word) %>% 
    sample_n(samplesize_values[i], replace = T) %>% 
    summarize(se = sd(Rating)/sqrt(length(Rating))) 
   
   colnames(sim_table)[1:length(unique(con_use$Word))] <- temp$Word
   sim_table[iterate, 1:length(unique(con_use$Word))] <- temp$se
   sim_table[iterate, "sample_size"] <- samplesize_values[i]
   sim_table[iterate, "nsim"] <- p
   
   iterate <- iterate + 1
  }
}

# figure out potential samples
final_sample_c <- 
 sim_table %>% 
 pivot_longer(cols = -c(sample_size, nsim)) %>% 
 rename(item = name, se = value) %>% 
 group_by(sample_size, nsim) %>% 
 summarize(Percent_Below10 = sum(se <= cutoff_c["10%"])/length(unique(con_use$Word)),
          Percent_Below20 = sum(se <= cutoff_c["20%"])/length(unique(con_use$Word)),
          Percent_Below30 = sum(se <= cutoff_c["30%"])/length(unique(con_use$Word)),
          Percent_Below40 = sum(se <= cutoff_c["40%"])/length(unique(con_use$Word)),
          Percent_Below50 = sum(se <= cutoff_c["50%"])/length(unique(con_use$Word)),
          Percent_Below60 = sum(se <= cutoff_c["60%"])/length(unique(con_use$Word)),
          Percent_Below70 = sum(se <= cutoff_c["70%"])/length(unique(con_use$Word)),
          Percent_Below80 = sum(se <= cutoff_c["80%"])/length(unique(con_use$Word)),
          Percent_Below90 = sum(se <= cutoff_c["90%"])/length(unique(con_use$Word))) %>% 
  ungroup() %>% 
  group_by(sample_size) %>% 
  summarize(Percent_Below10 = mean(Percent_Below10),
            Percent_Below20 = mean(Percent_Below20),
            Percent_Below30 = mean(Percent_Below30),
            Percent_Below40 = mean(Percent_Below40),
            Percent_Below50 = mean(Percent_Below50),
            Percent_Below60 = mean(Percent_Below60),
            Percent_Below70 = mean(Percent_Below70),
            Percent_Below80 = mean(Percent_Below80),
            Percent_Below90 = mean(Percent_Below90)) %>% 
  pivot_longer(-sample_size) %>% 
  filter(value >= .80) %>% 
  group_by(name) %>% 
  arrange(sample_size, value)

values_80_c <- final_sample_c %>% 
  group_by(name) %>% 
  arrange(sample_size, value) %>% 
  slice_head()

values_85_c <- final_sample_c %>% 
  filter(value >= .85) %>% 
  group_by(name) %>% 
  arrange(sample_size, value) %>% 
  slice_head()
  
values_90_c <- final_sample_c %>% 
  filter(value >= .90) %>% 
  group_by(name) %>% 
  arrange(sample_size, value) %>% 
  slice_head()
  
values_95_c <- final_sample_c %>% 
  filter(value >= .95) %>% 
  group_by(name) %>% 
  arrange(sample_size, value) %>% 
  slice_head()

# calculate the SD of the item's SD 
sd_items_c <- sd(tapply(con_use$Rating, con_use$Word, sd))
```

```{r elp-sample, include = F, echo = F}
# sequence of sample sizes to try
nsim <- 5
samplesize_values <- seq(20, 300, 5)
# create a blank table for us to save the values in 
sim_table <- matrix(NA, 
          nrow = length(samplesize_values), 
          ncol = length(unique(elp_use$Stimulus)))
# make it a data frame
sim_table <- as.data.frame(sim_table)
# add a place for sample size values 
sim_table$sample_size <- NA

iterate <- 1

for (p in 1:nsim){
  
  # loop over sample sizes
  for (i in 1:length(samplesize_values)){
    
   # temp dataframe that samples and summarizes
   temp <- elp_use %>% 
    group_by(Stimulus) %>% 
    sample_n(samplesize_values[i], replace = T) %>% 
    summarize(se = sd(RT)/sqrt(length(RT))) 
   
   colnames(sim_table)[1:length(unique(elp_use$Stimulus))] <- temp$Stimulus
   sim_table[iterate, 1:length(unique(elp_use$Stimulus))] <- temp$se
   sim_table[iterate, "sample_size"] <- samplesize_values[i]
   sim_table[iterate, "nsim"] <- p

   iterate <- iterate + 1
   }

}
# figure out potential samples
final_sample_e <- 
 sim_table %>% 
 pivot_longer(cols = -c(sample_size)) %>% 
 rename(item = name, se = value) %>% 
 group_by(sample_size, nsim) %>% 
 summarize(Percent_Below10 = sum(se <= cutoff_e["10%"])/length(unique(elp_use$Stimulus)),
          Percent_Below20 = sum(se <= cutoff_e["20%"])/length(unique(elp_use$Stimulus)),
          Percent_Below30 = sum(se <= cutoff_e["30%"])/length(unique(elp_use$Stimulus)),
          Percent_Below40 = sum(se <= cutoff_e["40%"])/length(unique(elp_use$Stimulus)),
          Percent_Below50 = sum(se <= cutoff_e["50%"])/length(unique(elp_use$Stimulus)),
          Percent_Below60 = sum(se <= cutoff_e["60%"])/length(unique(elp_use$Stimulus)),
          Percent_Below70 = sum(se <= cutoff_e["70%"])/length(unique(elp_use$Stimulus)),
          Percent_Below80 = sum(se <= cutoff_e["80%"])/length(unique(elp_use$Stimulus)),
          Percent_Below90 = sum(se <= cutoff_e["90%"])/length(unique(elp_use$Stimulus))) %>% 
  ungroup() %>% 
  group_by(sample_size) %>% 
  summarize(Percent_Below10 = mean(Percent_Below10),
            Percent_Below20 = mean(Percent_Below20),
            Percent_Below30 = mean(Percent_Below30),
            Percent_Below40 = mean(Percent_Below40),
            Percent_Below50 = mean(Percent_Below50),
            Percent_Below60 = mean(Percent_Below60),
            Percent_Below70 = mean(Percent_Below70),
            Percent_Below80 = mean(Percent_Below80),
            Percent_Below90 = mean(Percent_Below90)) %>% 
  pivot_longer(-sample_size) %>% 
  filter(value >= .80) %>% 
  group_by(name) %>% 
  arrange(sample_size, value)

values_80_e <- final_sample_e %>% 
  group_by(name) %>% 
  arrange(sample_size, value) %>% 
  slice_head()

values_85_e <- final_sample_e %>% 
  filter(value >= .85) %>% 
  group_by(name) %>% 
  arrange(sample_size, value) %>% 
  slice_head()
  
values_90_e <- final_sample_e %>% 
  filter(value >= .90) %>% 
  group_by(name) %>% 
  arrange(sample_size, value) %>% 
  slice_head()
  
values_95_e <- final_sample_e %>% 
  filter(value >= .95) %>% 
  group_by(name) %>% 
  arrange(value, sample_size) %>% 
  slice_head()

# calculate the SD of the item's SD 
sd_items_e <- sd(tapply(elp_use$RT, elp_use$Stimulus, sd, na.rm = T), na.rm = T)
```

```{r table-example, results = 'asis'}
# concreteness before - elp before 
  # sample size
  # decile cutoffs? 
  # number of participants for each decile? 
  # 80, 85, 90, 95 

concrete_cuts <- values_80_c %>% 
  left_join(values_85_c, by = "name") %>% 
  left_join(values_90_c, by = "name") %>% 
  left_join(values_95_c, by = "name") %>%
  ungroup() %>% 
  rename("c_80" = sample_size.x, 
         "c_85" = sample_size.y, 
         "c_90" = sample_size.x.x, 
         "c_95" = sample_size.y.y) %>% 
  select(c_80, c_85, c_90, c_95)

lexical_cuts <- values_80_e %>% 
  left_join(values_85_e, by = "name") %>% 
  left_join(values_90_e, by = "name") %>% 
  left_join(values_95_e, by = "name") %>%
  ungroup() %>% 
  rename("e_80" = sample_size.x, 
         "e_85" = sample_size.y, 
         "e_90" = sample_size.x.x, 
         "e_95" = sample_size.y.y) %>% 
  select(e_80, e_85, e_90, e_95)

results_table <- matrix(NA, nrow = 9, ncol = 11)
results_table[ , 1] <- c("Decile 10", "Decile 20", "Decile 30", 
                         "Decile 40", "Decile 50", "Decile 60", 
                         "Decile 70", "Decile 80", "Decile 90")
results_table[1 , 2:11] <- unlist(c(cutoff_c[1], concrete_cuts[1 , ], 
                                    cutoff_e[1], lexical_cuts[1, ]))
results_table[2 , 2:11] <- unlist(c(cutoff_c[2], concrete_cuts[2 , ], 
                                    cutoff_e[2], lexical_cuts[2, ]))
results_table[3 , 2:11] <- unlist(c(cutoff_c[3], concrete_cuts[3 , ], 
                                    cutoff_e[3], lexical_cuts[3, ]))
results_table[4 , 2:11] <- unlist(c(cutoff_c[4], concrete_cuts[4 , ], 
                                    cutoff_e[4], lexical_cuts[4, ]))
results_table[5 , 2:11] <- unlist(c(cutoff_c[5], concrete_cuts[5 , ], 
                                    cutoff_e[5], lexical_cuts[5, ]))
results_table[6 , 2:11] <- unlist(c(cutoff_c[6], concrete_cuts[6 , ], 
                                    cutoff_e[6], lexical_cuts[6, ]))
results_table[7 , 2:11] <- unlist(c(cutoff_c[7], concrete_cuts[7 , ], 
                                    cutoff_e[7], lexical_cuts[7, ]))
results_table[8 , 2:11] <- unlist(c(cutoff_c[8], concrete_cuts[8 , ],
                                    cutoff_e[8], lexical_cuts[8, ]))
results_table[9 , 2:11] <- unlist(c(cutoff_c[9], concrete_cuts[9 , ], 
                                 cutoff_e[9], lexical_cuts[9, ]))

results_table_example <- as.data.frame(results_table)
colnames(results_table_example) <- c("Deciles", "C SE", "C 80", "C 85", "C 90", "C 95",
             "L SE", "L 80", "L 85", "L 90", "L 95")
results_table_example$`C Decile SE` <- apa_num(as.numeric(results_table_example$`C Decile SE`), digits = 2)
results_table_example$`L Decile SE` <- apa_num(as.numeric(results_table_example$`L Decile SE`), digits = 2)

apa_table(results_table_example, 
          caption = "Sample Size Estimates by Decile for Example Study",
          note = "C = Concreteness rating, L = Lexical Decision Response Latencies. Estimates are based on meeting at least the minimum percent of items (e.g., 80%) but may be estimated over that amount (e.g., 82.5%). SE columns represent the standard error value cutoff for each decile, while 80/85/90/95 percent columns represent the sample size needed to have that percent of items below the SE cutoff. For example, 100 participants are required to ensure at least 95% of concreteness items SE are below the 10 percent decile SE cutoff, and 175 participants are necessary for the lexical decision SE to be below its 10 percent decile cutoff.")
```

*Step 6*. In the last step, the researcher would indicate their smallest sample size, the cutoff standard error criterion if they wanted to adaptively test (e.g., examine the SE after each participant and stop data collection if all items reached criteria), and their maximum sample size. As mentioned earlier, the decile for a balanced standard error cutoff is unclear and without guidance, a potential set of researcher degrees of freedom could play a role in the chosen cutoff [CITE SIMONSOHN]. Even though both measurements (ratings and response latencies) appear to converge on similar sample size suggestions for each decile and percent level, the impact of scale size (i.e., concreteness ratings 1-5 versus response latencies in ms 0-`r max(elp_small$RT, na.rm = T)`) and heterogeneity of item standard errors (concrete $SD_{SD}$ = `r apa_num(sd_items_c)` and lexical $SD_{SD}$ = `r apa_num(sd_items_e)`) is not obvious. Last, by selecting the ends of the distribution for our concreteness words, skew of the distribution may additionally impact our estimates. Each of these will be explored in our simulation.

# Simulation Method

In order to evaluate our approach, we used data simulation to create representative pilot datasets of several popular cognitive scales (1-7 measurements, 0-100 percentage measurements, and 0-3000 response latency type scale data). For each of these scales, we also manipulated item heterogeneity by simulating small differences in item variances to large differences in item variances based on original scale size. On each of the simulated datasets, we applied the above proposed method to determine how the procedure would perform and evaluated what criteria should be used for cutoff selection (Step 2). This procedure was performed on distributions in the middle of the scale (i.e., normal) and at the ceiling of the scale (i.e., skewed). With this simulation, we will answer several questions:

1) How do pilot data influence sample size suggestions?

 A. How does scale size impact sample size estimations? In theory, the size of the scale used should not impact the power estimates; however, larger scales have a potential for more variability in their item standard deviations (see point C).

 B. How does distribution skew impact sample size estimations? Skew can potentially decrease item variance heterogeneity (i.e., all items are at ceiling, and therefore, variance between item standard errors is low) or could increase heterogeneity (i.e., some items are skewed, while others are not). Therefore, we expect skew to impact the estimates in the same way as point C. 

 C. How does heterogeneity impact sample size estimations? Heterogeneity should decrease power [@alexander1994; @rheinheimer2001], and thus, increased projected sample sizes should be proposed as heterogeneity of item variances increases.

2) Do the results match what one might expect for traditional power curves? Power curves are asymptotic; that is, they "level off" as sample size increases. Therefore, we expect that our procedure should also demonstrate a leveling off effect as pilot data sample size increases. For example, if one has a 500-person pilot study, our simulations should suggest a point at which items are likely measured well, which may have happened well before 500.

3) What should the suggested cutoff standard error decile be?

## Data Simulation

Table \@ref(tab:table-sim) presents the variables and information about the simulations as a summary.

```{r table-sim, results = 'asis'}
apa_table(data.frame(
  Information = c("Minimum", "Maximum", "$\\mu$", "$Skewed \\mu$", "$\\sigma_{\\mu}$", 
                  "$\\sigma$", "Small $\\sigma_{\\sigma}$", 
                  "Medium $\\sigma_{\\sigma}$", "Large $\\sigma_{\\sigma}$"),
  Likert = c(1, 7, 4, 6, .25, 2, .2, .4, .8),
  Percent = c(0, 100, 50, 85, 10, 25, 4, 8, 16),
  Milliseconds = c(0, 3000, 1000, 2500, 150, 400, 50, 100, 200)
), caption = "Parameter Values for Data Simulation",
escape = FALSE
)

```

*Population*. We simulated data for 30 items using the rnorm function assuming a normal distribution. Each item's population data was simulated with 1000 data points. Items were rounded to the nearest whole number to mimic scales generally collected by researchers. Items were also rounded to their appropriate scale endpoints (i.e., all items below 0 on a 1-7 scale were replaced with 1, etc.).

*Data Scale*. First, the scale of the data was manipulated by creating three sets of scales. The first scale was mimicked after small rating scales (i.e., 1-7 Likert-type style, treated as interval data) using a $\mu$ = 4 with a $\sigma$ = .25 around the mean to create item mean variability. The second scale included a larger potential distribution of scores with a $\mu$ = 50 ($\sigma$ = 10) imitating a 0-100 scale. Last, the final scale included a $\mu$ = 1000 ($\sigma$ = 150) simulating a study that may include response latency data in the milliseconds. For the skewed distributions, the item means were set to $\mu$ = 6, 85, and 2500 respectively with the same $\sigma$ values around the item means. Although there are many potential scales, these three represent a large number of potential variables commonly used in the social sciences. As we are suggesting item variances is a key factor for estimating sample sizes, the scale of the data is influential on the amount of potential variance. Smaller data ranges (1-7) cannot necessarily have the same variance as larger ranges (0-100).

*Item Heterogeneity*. Next, item heterogeneity was included by manipulating the potential for each individual item. For small scales, the $\sigma$ = 2 points with a variability of .2, .4, and .8 for low, medium, and high heterogeneity in the variances between items. For the medium scale of the data, $\sigma$ = 25 with a variance of 4, 8, and 16. Finally, for the large scale of the data, $\sigma$ = 400 with a variance of 50, 100, and 200 for heterogeneity. These values were based on the proportion of the overall scale and potential variance. 

*Pilot Data Samples*. Each of the populations shown in Table \@ref(tab:table-sim) was then sampled as if a researcher was conducting a pilot study. The sample sizes started at 20 participants per item, increasing in units of 10 up to 100 participants. Each of these samples would correspond to Step 1 of the proposed method where a researcher would use pilot data to start their estimation. Therefore, the simulations included 3 scales X 3 heterogeneity values X 2 normal/skewed distributions X 9 pilot sample sizes representing a potential Step 1 of our procedure.

## Researcher Sample Simulation

In this section, we simulate what a researcher might do if they follow our suggested application of AIPE to sample size planning based on well measured items. Assuming that each pilot sample represents a dataset that a researcher has collected (Step 1), the standard errors for each item were calculated to mimic the AIPE procedure of finding an appropriately small confidence interval, as SE functions as the main component of the formula for normal distribution confidence intervals. Standard errors were calculated at each decile of the items up to 90% (i.e., 0% smallest SE, 10% ..., 90% largest SE). The lower deciles would represent a strict criterion for accurate measurement, as many items would need smaller SEs to meet cutoff scores, while the higher deciles would represent less strict criteria for cutoff scores(Step 2).

We then simulated samples of 20 to 2000 increasing in units of 20 to determine what the new sample size suggestion would be (Step 3). We assume that samples over 500 may be considered too large for many researchers who do not work in teams or have participant funds. However, the sample size simulations were estimated over this amount to determine the pattern of suggested sample sizes (i.e., the function between original sample size and projected sample size).

Next, the percentage of items that fall below the cutoff scores and therefore would be considered "well-measured" were calculated for each decile by sample (Step 4). From these data, we pinpoint the smallest suggested sample size at which 80%, 85%, 90% and 95% of the items fall below the cutoff criterion (Step 5). These values were chosen as popular measures of "power" in which one could determine the minimum suggested sample size (potentially 80% of the items) and the maximum suggested sample size (selected from a higher percentage, such as 90% or 95%).

In order to minimize the potential for random quirks to arise, we simulated the sample selection from the population 100 times and the researcher simulation 100 times for each of those selections. This resulted in 1,620,000 simulations of all combinations of variables (i.e., scale of the data, heterogeneity, data skew, pilot study size, researcher simulation size). The average of these simulations is presented in the results.

# Simulation Results

```{r import-data}
summary_long <- bind_rows(
  import("simulation_study/simulation_middle/simulated_summary_data_middle.csv") %>% 
    mutate(data_type = "Normal"), 
    import("simulation_study/simulation_ceiling/simulated_summary_data_ceiling.csv") %>% 
    mutate(data_type = "Ceiling"))

summary_long$scale_size <- factor(summary_long$scale_size,
                                  levels = c("Likert", "Percent", "Milliseconds"))
summary_long$variability <- factor(summary_long$variability, 
                                   levels = c("Small Heterogeneity", 
                                              "Medium Heterogeneity", 
                                              "Large Heterogeneity"))

### make a decision here ### 
summary_long <- summary_long %>% filter(name != "Decile 0" & name != "Decile 10" & name != "Decile 20")
```

## Pilot Data Influence on Sample Size

For each variable, the plot of the pilot sample size, projected sample size (i.e., what the simulation suggested), and power levels are presented below. The large number of variables means we cannot plot them all simultaneously, and therefore, we averaged the results across other variables for each plot. The entire datasets can be examined on our OSF page.

### Scale Size

```{r scale-size-figure, echo = F, fig.cap="Simulated pilot sample size and final projected sample size to achieve 80%, 85%, 90%, and 95% of items below threshold. These values are averaged over all other variables including decile. Black dots represent original sample size for reference. "}

percent_var <- sqrt(((100-0)^2)/4)
millisecond_var <- sqrt(((3000-0)^2)/4)

summary_long$percent_var <- ifelse(
  summary_long$scale_size == "Likert", summary_long$sd_item/likert_var, ifelse(
    summary_long$scale_size == "Percent", summary_long$sd_item/percent_var, 
      summary_long$sd_item/millisecond_var
  )
)

mean_var <- tapply(summary_long$percent_var, summary_long$scale_size, mean)

# New facet label names for power variable
power.labs <- c("80% of items", "85% of items",
                "90% of items", "95% of items")
names(power.labs) <- c("80", "85", "90", "95")

plot_colors <- c("#4FEAAC", "#FF826F", "#4F84FD")

ggplot(summary_long %>% 
         group_by(original_n, power, scale_size) %>% 
         summarize(sample_size_avg = mean(sample_size)),
       aes(original_n, sample_size_avg, color = scale_size)) + 
  geom_point() + 
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  geom_line(aes(original_n, original_n), color = "black") + 
  theme_linedraw(base_size = 15) + 
  xlab("\nPilot Sample Size") + 
  ylab("Projected Sample Size\n") + 
  scale_color_manual(name = "Scale Size",
                     values = c(plot_colors)) +
  facet_wrap(~power,
             labeller = labeller(power = power.labs))+
  theme_bw()
```

Figure \@ref(fig:scale-size-figure) demonstrates the influence of scale size on the results separated by potential cutoff level. The black dots denote the original sample size for reference. Larger scales have more potential variability, and therefore, we see that percent and millisecond scales project a larger required sample size. This relationship does not appear to be linear with scale size, as percent scales often represent the highest projected sample size. Potentially, this finding is due to the larger proportion of possible variance -- the variance of the item standard deviations / total possible variance -- was largest for percent scales in this set of simulations ($p_{Percent}$ = `r apa_num(mean_var["Percent"], gt1 = FALSE)`). This finding may be an interaction with heterogeneity, as the Likert scale had the next highest percent variability in item standard errors ($p_{Likert}$ = `r apa_num(mean_var["Likert"], gt1 = FALSE)`), followed by milliseconds ($p_{Milliseconds}$ = `r apa_num(mean_var["Milliseconds"], gt1 = FALSE)`).

### Skew

```{r scale-skew-figure, echo = F, fig.cap="Simulated pilot sample size and final projected sample size to achieve 80%, 85%, 90%, and 95% of items below threshold. In comparison to Figure 1, this figure shows projected sample size for ceiling versus normal distributions on each scale. All other variables are averaged together, and black dots represent original sample size for reference." }

ggplot(summary_long %>% 
         group_by(original_n, power, data_type) %>% 
         summarize(sample_size_avg = mean(sample_size)),
       aes(original_n, sample_size_avg, color = data_type)) + 
  geom_point() + 
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  geom_line(aes(original_n, original_n), color = "black") + 
  theme_linedraw(base_size = 15) + 
  xlab("\nPilot Sample Size") + 
  ylab("Projected Sample Size\n") + 
  scale_color_manual(name = "Scale Skew",
                     values = c(plot_colors)) +
  facet_wrap(~power,
             labeller = labeller(power = power.labs))+
  theme_bw()
```

Figure \@ref(fig:scale-skew-figure) displays that ceiling distributions, averaged over all other variables, show slightly higher estimates than normal distributions. This result is consistent across scale type and heterogeneity, as results indicated that they are often the same or slightly higher for ceiling distributions.

### Item Heterogeneity

```{r scale-hetero-figure, echo = F, fig.cap="Simulated pilot sample size and final projected sample size to achieve 80%, 85%, 90%, and 95% of items below threshold. In comparison to Figure 1 and 2, this figure shows projected sample size or differing amounts of heterogeneity on each scale. All other variables are averaged together, and black dots represent original sample size for reference."}

ggplot(summary_long %>% 
         group_by(original_n, power, variability) %>% 
         summarize(sample_size_avg = mean(sample_size)),
       aes(original_n, sample_size_avg, color = variability)) + 
  geom_point() + 
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line() + 
  theme_linedraw() + 
  xlab("Pilot Sample Size") + 
  ylab("Projected Sample Size") + 
  scale_color_discrete(name = "Item Heterogeneity") +
  facet_wrap(~power)
```

```{r strongest-predict, include = F, echo = F}
regression_DF <- summary_long %>% 
  group_by(original_n, scale_size, variability, data_type) %>% 
  summarize(sample_size = mean(sample_size),
            sd_item = mean(sd_item),
            percent_var = mean(percent_var))
model.samplesize <- lm(sample_size ~ original_n + scale_size + percent_var + data_type,
                       data = regression_DF)
summary(model.samplesize, correlation = TRUE)
```

Figure \@ref(fig:scale-hetero-figure) displays the results for item heterogeneity for different levels of potential power. In this figure, we found that our suggested procedure does capture the differences in heterogeneity. As heterogeneity increases in item variances, the proposed sample size also increases.

```{r table-predict-largest, results='asis'}
temp_table <- broom::tidy(model.samplesize)
temp_table$term <- c("Intercept", "Pilot Sample Size", "Scale: Likert v Percent", "Scale: Likert v Milllisecond", "Proportion Variability", "Data: Ceiling v Normal")
temp_table$p.value <- apa_p(temp_table$p.value)
temp_table$pr <- apa_p((temp_table$statistic / sqrt(temp_table$statistic^2 + model.samplesize$df.residual))^2)

apa_table(
  temp_table, 
  col.names = c("Term", "Estimate", "$SE$", "$t$", "$p$", "$pr^2$"),
  caption = "Prediction of Proposed Sample Size from Simulated Variables"
)
```

Using a regression model, we predicted proposed sample size using pilot sample size, scale size, proportion variability (i.e., heterogeneity), and data type (normal, ceiling). As shown in Table \@ref(tab:table-predict-largest), the largest influence on proposed sample size is the original pilot sample size, followed by proportion of variance/heterogeneity, and then data and scale sizes.

## Projected Sample Size Sensitivity to Pilot Sample Size

In our second question, we examined if the suggested procedure was sensitive to the amount of information present in the pilot data. Larger pilot data is more informative, and therefore, we should expect a lower projected sample size. As shown in each figure presented already, we do not find this effect. These simulations from the pilot data would nearly always suggest a larger sample size - mostly in a linear trend increasing with sample sizes. This result comes from the nature of the procedure - if we base our estimates on a SE cutoff, we will almost always need a bit more people for items to meet those goals. This result does not achieve our second goal.

Therefore, we suggest using a correction factor on the simulation procedure to account for the known asymptotic nature of power (i.e., at larger sample sizes power increases level off). For this function in our simulation study, we combined a correction factor for upward biasing of effect sizes (Hedges' correction) with the formula for exponential decay calculations. The decay factor was calculated as follows:

$$ 1 - \sqrt{\frac{N_{Pilot} - min(N_{Simulation})}{N_{Pilot}}}^{log_2(N_{Pilot})}$$

$N_{Pilot}$ indicates the sample size of the pilot data minus the minimum simulated sample size to ensure that the smallest sample sizes do not decay (i.e., the formula zeroes out). This value is raised to the power of $log_2$ of the sample size of the pilot data, which decreases the impact of the decay to smaller increments for increasing sample sizes. This value is then multiplied by the projected sample size. As shown in Figure \@ref(fig:corrected-figure), this correction factor produces the desired quality of maintaining that small pilot studies should *increase* sample size, and that sample size suggestions level off as pilot study data sample size increases.

```{r corrected-figure, echo = F, fig.cap="Corrected projected sample sizes for variability and power levels to achieve 80%, 85%, 90%, and 95% of items below threshold. All other variables are averaged together, and black dots represent original sample size for reference."}

decay <- 1-sqrt((summary_long$original_n-20)/summary_long$original_n)^log2(summary_long$original_n)
summary_long$new_sample <- summary_long$sample_size*decay
ggplot(summary_long %>% 
         group_by(original_n, power, variability) %>% 
         summarize(sample_size_avg = mean(sample_size),
                   new_sample_size_avg = mean(new_sample)),
       aes(original_n, new_sample_size_avg, color = variability)) + 
  geom_line(aes(original_n, original_n), color = "black") +
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_point() + 
  geom_line() + 
  xlab("\nPilot Sample Size") + 
  ylab("Projected Sample Size\n") + 
  scale_color_manual(name = "Item Heterogeneity",
                     values = c(plot_colors)) +
  facet_wrap(~power,
             labeller = labeller(power = power.labs))+
  theme_bw()
```

## Corrections for Individual Researchers

We have portrayed that this procedure, with a correction factor, can perform as desired. However, within real scenarios, researchers will only have one pilot sample, not the various simulated samples shown above. What should the researcher do to correct their projected sample size from their own pilot data simulations?

To explore if we could recover the new projected sample size from data a researcher would have, we used regression models to create a formula for researcher correction. The researcher employing our procedure would have the possible following variables from their simulations on their (one) pilot dataset: 1) proposed sample size, 2) pilot sample size, 3) estimate of heterogeneity for the items, 4) and the estimated percent of items below the threshold. Given the non-linear nature of the correction, we added each variable and its non-linear `log2` transform to the regression equation, as this function was used to create the correction. The intercept only model was used as a starting point (i.e., `corrected sample ~ 1`), and then all eight variables (each variable and their `log2` transform) were entered into a forward stepwise regression to capture the corrected scores with the most predictive values. Each variable was entered one at a time using the `step` function from the *stats* library in *R* [@R-base].

```{r regressions, include = F, echo = F}
intercept_only <- lm(new_sample ~ 1, data = summary_long)
user_model3 <- lm(new_sample ~ sample_size + percent_var + original_n + power + log2(sample_size) + log2(percent_var) + log2(original_n) + log2(power), data = summary_long)

forward <- step(intercept_only, direction='forward', scope=formula(user_model3), trace=0)

model.summary <- tidy(forward)
model.summary$AIC <- forward$anova$AIC
```

```{r test-assume, include = F, echo = F}
resid_model <- scale(residuals(forward))
fit_model <- scale(fitted.values(forward))
hist(resid_model)
qqnorm(resid_model); abline(0,1)
plot(fit_model, resid_model); abline(v = 0); abline(h = 0)
# definitely residuals are a mess, but this also includes all deciles
```

As shown in Table \@ref(tab:table-correction), all variables were included in the final equation, each contributing a significant change to the previous model, as defined by $\Delta$AIC \> 2 points change between each step of the model. Proposed sample size and original sample size were the largest predictors -- unsurprising given the correction formula employed -- followed by the percent "power" level and proportion of variance. This formula approximation captures `r apa_print(forward)$estimate$modelfit$r2` of the variance in sample size scores and should allow a researcher to estimate based on their own data, `r apa_print(forward)$statistic$modelfit$r2`. We provide convenience functions in our additional materials to assist researchers in estimating the final adjusted sample size.

```{r table-correction, results = 'asis'}
table_correct <- model.summary
table_correct$term <- c("Intercept", "Projected Sample Size", "Pilot Sample Size", 
                        "Log2 Projected Sample Size", "Log2 Pilot Sample Size", 
                        "Log2 Power", "Proportion Variability", 
                        "Log2 Proportion Variability", "Power")
table_correct$estimate <- printnum(table_correct$estimate, digits = 3)
table_correct$std.error <- printnum(table_correct$std.error, digits = 3)
table_correct$statistic <- printnum(table_correct$statistic, digits = 3)
table_correct$p.value <- printnum(table_correct$p.value, digits = 3,
                 zero = FALSE, gt1 = F)
table_correct$AIC <- printnum(table_correct$AIC, digits = 2)
apa_table(table_correct, 
     col.names = c("Term", "Estimate", "$SE$", "$t$", "$p$", "AIC"),
     caption = "Parameters for All Decile Cutoff Scores")
```

## Choosing an Appropriate cutoff

```{r R2-cutoff, include = F, echo = F}
by_cutoff <- list()
R2 <- list()
for (cutoff in unique(summary_long$name)){
 by_cutoff[[cutoff]] <- lm(new_sample ~ sample_size + percent_var + original_n + power + log2(sample_size) + log2(percent_var) + log2(original_n) + log2(power), data = summary_long %>% filter(name == cutoff))
 R2[cutoff] <- summary(by_cutoff[[cutoff]])$r.squared
}
```

```{r test-decile, include = F, echo = F}
resid_model <- scale(residuals(by_cutoff[["Decile 40"]]))
fit_model <- scale(fitted.values(by_cutoff[["Decile 40"]]))
# hist(resid_model)
# qqnorm(resid_model); abline(0,1)
# plot(fit_model, resid_model); abline(v = 0); abline(h = 0)
# by scale, power, source
separate_lms <- list()
R2_lms <- list()
for (scale_loop in unique(summary_long$scale_size)){
 for (power_loop in unique(summary_long$power)){
  for (source_loop in unique(summary_long$variability)){
   temp <- summary_long %>% 
    filter(scale_size == scale_loop) %>% 
    filter(power == power_loop) %>% 
    filter(variability == source_loop) %>% 
    filter(name == "Decile 50")
   
   separate_lms[[paste(scale_loop, power_loop, source_loop, sep = "_")]] <- lm(new_sample ~ sample_size + sd_item + original_n, data = temp)
   R2_lms[[paste(scale_loop, power_loop, source_loop, sep = "_")]] <- summary(separate_lms[[paste(scale_loop, power_loop, source_loop, sep = "_")]])$r.squared
  }
 }
}
# resid_model <- scale(residuals(separate_lms[[10]]))
# fit_model <- scale(fitted.values(separate_lms[[10]]))
# hist(resid_model)
# qqnorm(resid_model); abline(0,1)
# plot(fit_model, resid_model); abline(v = 0); abline(h = 0)
```

Last, we examined the question of an appropriate SE decile. First, the 0%, 10%, and 20% deciles are likely too restrictive, providing very large estimates that do not always find a reasonable sample size in proportion to the pilot sample size, scale size, and heterogeneity. If we examine the $R^2$ values for each decile of our regression equation separately, we find that the values are all $R^2$ \> .99 with very little differences between them. Figures \@ref(fig:decile40-figure) and \@ref(fig:decile50-figure) illustrate the corrected scores for simulations at the 40% and 50% decile recommended cutoff for item standard errors. For small heterogeneity, differences in decile are minimal, while larger heterogeneity shows more correction at the 40% decile range, especially for scales with larger potential variance. Therefore, we would suggest the 40% decile to overpower each item to determine the minimum and maximum sample size.

```{r decile40-figure, fig.cap = "Comparison of the cutoffs for 40% deciles across heterogeneity (columns), powering of items (rows), and scale size (color)."}
summary_long %>% 
  filter(name == "Decile 40") %>% 
  group_by(variability, original_n, scale_size, name, power) %>% 
  summarize(new_sample = mean(new_sample)) %>% 
  ggplot(., aes(original_n, new_sample, color = scale_size)) + 
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line(aes(original_n, original_n), color = "black") + 
  geom_point() + 
  geom_line() + 
  theme_classic(base_size = 10) + 
  facet_grid(power~ variability,
             labeller = labeller(power = power.labs)) +
  ggtitle("Results for 40% deciles") +
  xlab("\nPilot Sample Size") + 
  ylab("Corrected Sample Size\n") + 
  scale_color_manual(name = "Scale Size",
                     values = c(plot_colors)) + 
  theme_bw() +
  theme(legend.position="bottom")

```

```{r decile50-figure, fig.cap = "Comparison of the cutoffs for 50% deciles across heterogeneity (columns), powering of items (rows), and scale size (color)."}
summary_long %>% 
  filter(name == "Decile 50") %>% 
  group_by(variability, original_n, scale_size, name, power) %>% 
  summarize(new_sample = mean(new_sample)) %>% 
  ggplot(., aes(original_n, new_sample, color = scale_size)) + 
  geom_point(aes(original_n, original_n), color = "black") + 
  geom_line(aes(original_n, original_n), color = "black") + 
  geom_point() + 
  geom_line() +
  theme_classic(base_size = 10) + 
  facet_grid(power~ variability,
             labeller = labeller(power = power.labs)) + 
  ggtitle("Results for 40% deciles") +
  xlab("\nPilot Sample Size") + 
  ylab("Corrected Sample Size\n") + 
  scale_shape_discrete(name = "Scale Size") +
  scale_color_manual(name = "Cutoff Decile",
                     values = c(plot_colors)) + 
  theme_bw() +
  theme(legend.position="bottom")

```

The final formula for 40% decile correction is provided in Table \@ref(tab:table-decile). Proportion of variance can be calculated with the following:

$$\frac{SD_{Item SD}}{\sqrt{\frac{(Maximum - Minimum)^2}{4}}}$$ where maximum and minimum are the max and min values found in the scale (or the data, if the scale is unbounded). This formula would be applied in Step 5 of the proposed procedure. While the estimated coefficients could change given variations on our simulation parameters, the general size and pattern of coefficients was consistent, and therefore, we believe this correction equation should work for a variety of use cases. We will now demonstrate the final procedure on the example provided earlier.

```{r table-decile, results = 'asis'}
table_correct <- tidy(summary(by_cutoff$`Decile 40`))
table_correct <- table_correct[c(1, 2, 4, 6, 8, 9, 3, 7, 5) , ]
table_correct$term <- c("Intercept", "Projected Sample Size", "Pilot Sample Size", 
                        "Log2 Projected Sample Size", "Log2 Pilot Sample Size", 
                        "Log2 Power", "Proportion Variability", 
                        "Log2 Proportion Variability", "Power")
table_correct$estimate <- printnum(table_correct$estimate, digits = 3)
table_correct$std.error <- printnum(table_correct$std.error, digits = 3)
table_correct$statistic <- printnum(table_correct$statistic, digits = 3)
table_correct$p.value <- printnum(table_correct$p.value, digits = 3,
                 zero = FALSE, gt1 = F)
apa_table(table_correct, 
     col.names = c("Term", "Estimate", "$SE$", "$t$", "$p$"),
     caption = "Parameters for 40% Decile Cutoff Scores")
```

# Updated Example

```{r table-scores-updated, results = 'asis', eval = F}
c_sample <- c(results_table_example$`C 80`[4], 
              results_table_example$`C 85`[4],
              results_table_example$`C 90`[4],
              results_table_example$`C 95`[4])

e_sample <- c(results_table_example$`L 80`[4], 
              results_table_example$`L 85`[4],
              results_table_example$`L 90`[4],
              results_table_example$`L 95`[4])

con_prop <- sd_items_c / sqrt(((max(con_use$Rating) - min(con_use$Rating))^2)/4)
ldt_prop <- sd_items_e / sqrt(((max(elp_use$RT) - min(elp_use$RT))^2)/4)

table_correction_applied <- matrix(NA, ncol = 10, nrow = 9)
table_correction_applied[1 , ] <- c("Formula", as.numeric(table_correct$estimate))
table_correction_applied[2 , ] <- c("Concrete 80", 1, c_sample[1], 
                                    pilot_size_c, log2(as.numeric(c_sample[1])), 
                                    log2(pilot_size_c), log2(80), con_prop, 
                                    log2(con_prop), 80)

table_correction_applied[3 , ] <- c("Concrete 85", 1, c_sample[2], 
                                    pilot_size_c, log2(as.numeric(c_sample[2])), 
                                    log2(pilot_size_c), log2(85), con_prop, 
                                    log2(con_prop), 85)

table_correction_applied[4 , ] <- c("Concrete 90", 1, c_sample[3], 
                                    pilot_size_c, log2(as.numeric(c_sample[3])), 
                                    log2(pilot_size_c), log2(90), con_prop, 
                                    log2(con_prop), 90)

table_correction_applied[5 , ] <- c("Concrete 95", 1, c_sample[4], 
                                    pilot_size_c, log2(as.numeric(c_sample[4])), 
                                    log2(pilot_size_c), log2(95), con_prop, 
                                    log2(con_prop), 95)

table_correction_applied[6 , ] <- c("LDT 80", 1, e_sample[1], 
                                    pilot_size_e, log2(as.numeric(e_sample[1])), 
                                    log2(pilot_size_e), log2(80), ldt_prop, 
                                    log2(ldt_prop), 80)

table_correction_applied[7 , ] <- c("LDT 85", 1, e_sample[2], 
                                    pilot_size_e, log2(as.numeric(e_sample[2])), 
                                    log2(pilot_size_e), log2(85), ldt_prop, 
                                    log2(ldt_prop), 85)

table_correction_applied[8 , ] <- c("LDT 90", 1, e_sample[3], 
                                    pilot_size_e, log2(as.numeric(e_sample[3])), 
                                    log2(pilot_size_e), log2(90), ldt_prop, 
                                    log2(ldt_prop), 90)

table_correction_applied[9 , ] <- c("LDT 95", 1, e_sample[4], 
                                    pilot_size_e, log2(as.numeric(e_sample[4])), 
                                    log2(pilot_size_e), log2(95), ldt_prop, 
                                    log2(ldt_prop), 95)

table_correction_applied <- as.data.frame(table_correction_applied) 
colnames(table_correction_applied) <- c("Formula Estimate", "Intercept", 
                                        "Projected SS", "Pilot SS", 
                                        "Log2 Projected SS", "Log2 Pilot SS", 
                                        "Log2 Power", "Prop Var", 
                                        "Log2 Prop Var", "Power")

table_correction_applied <- table_correction_applied %>% 
  mutate_at(.vars = 2:10, as.numeric) %>% 
  mutate(`Corrected Sample` = NA)

for (i in 2:nrow(table_correction_applied)){
  table_correction_applied$`Corrected Sample`[i] <- 
    sum(table_correction_applied[1, -c(1, ncol(table_correction_applied))] * table_correction_applied[i, -c(1, ncol(table_correction_applied))] )
}

table_correction_applied$`Data Loss` <- NA
table_correction_applied$`Data Loss`[2:5] <- unlist(table_correction_applied$`Corrected Sample`[2:5]) * (1/mean(data_loss_c$percent_correct))

table_correction_applied$`Data Loss`[6:9] <- unlist(table_correction_applied$`Corrected Sample`[6:9]) * (1/mean(data_loss_e$percent_correct))

apa_table(table_correction_applied, 
          caption = "Applied Correction for Each Proposed Sample Size", 
          font_size = "footnotesize")
```

The updated proposal steps are in Table \@ref(tab:table-summary) on the right hand side. The main change occurs in Step 2 with a designated cutoff decile, and Step 5 with a correction score. Using the data from the 40% decile in Table \@ref(tab:table-example), we can determine that the stopping rule for concreteness ratings would be `r apa_num(cutoff_c[4])`, and the stopping rule for lexical decision times would be `r apa_num(cutoff_e[4])`. For Step 5, we apply our correction formula separately for each one, as they have different variability scores, and these scores are shown in Table \@ref(tab:table-scores-updated). Each row was multiplied by row one's formula, and then these scores are summed for the final sample size. Sample sizes cannot be proportional, so we recommend rounding up to the nearest whole number.

For one additional consideration, we calculated the potential amount of data retention given that participants could indicate they did not know a word ($M_{answered}$ = `r round(mean(data_loss_c$percent_correct), digits = 2)`, *SD* = `r round(sd(data_loss_c$percent_correct), digits = 2)`) in the concreteness task or answer a trial incorrectly in the lexical decision task ($M_{correct}$ = `r round(mean(data_loss_e$percent_correct), digits = 2)`, *SD* = `r round(sd(data_loss_e$percent_correct), digits = 2)`). In order to account for this data loss, the potential sample sizes were multiplied by $\frac{1}{p_{retained}}$ where the denominator is proportion retained for each task.

# Additional Materials

## Package

We have developed functions to implement the suggested procedure as part of an upcoming package `semanticprimeR`. You can install the package from GitHub using: `devtools::install_github("SemanticPriming/semanticprimeR")`. We detail the functions below with proposed steps in the process.

*Step 1*. Ideally, researchers would have pilot data that represented their proposed data collection. This data should be formatted in long format wherein each row represents the score from an item by participant, rather than wide format wherein each column represents an item and each row represents a single participant. The `tidyr::pivot_longer()` or `reshape::melt()` functions can be used to reformat wide data. If no pilot data is available, the `simulate_population()` function can be used with the following arguments (and example numbers, \* indicates optional). This function will return a dataframe with the simulated normal values for each item.

```{r sim_pop, include = T, echo = T}
# devtools::install_github("SemanticPriming/semanticprimeR")
library(semanticprimeR)
pops <- simulate_population(mu = 4, # item means
  mu_sigma = .2, # variability in item means 
  sigma = 2, # item standard deviations
  sigma_sigma = .2, # standard deviation of the standard deviations
  number_items = 30, # number of items
  number_scores = 20, # number of participants
  smallest_sigma = .02, #* smallest possible standard deviation
  min_score = 1, #* minimum score for truncating purposes
  max_score = 7, #* maximum score for truncating purposes
  digits = 0) #* number of digits for rounding
  
head(pops)
```

*Step 2*. In step 2, we can use `calculate_cutoff()` to calculate the standard error of the items, the standard deviation of the standard errors and the corresponding proportion of variance possible, and the 40% decile cutoff score. The `pops` dataframe can be used in this function, which has columns named `item` for the item labels (i.e., 1, 2, 3, 4 or characters can be used), and `score` for the dependent variable. This function returns a list of values to be used in subsequent steps.

```{r calc_cut, include = T, echo = T}
cutoff <- calculate_cutoff(population = pops, # pilot data or simulated data
  grouping_items = "item", # name of the item indicator column
  score = "score", # name of the dependent variable column
  minimum = 1, # minimum possible/found score
  maximum = 7) # maximum possible/found score
                           
cutoff$se_items # all standard errors of items
cutoff$sd_items # standard deviation of the standard errors
cutoff$cutoff # 40% decile score
cutoff$prop_var # proportion of possible variance 
```

*Step 3*. The `bootstrap_samples()` function creates bootstrapped samples from the pilot or simulated population data to estimate the number of participants needed for item standard error to be below the cutoff calculated in Step 2. This function returns a list of samples with sizes that start at the `start` size, increase by `increase`, and end with the `stop` sample size. The population or pilot data will be included in `population`, and the item column indicator should be included in `grouping_items`. The `nsim` argument determines the number of bootstrapped simulations to run. 

```{r boot-sample, include = T, echo = T}
samples <- bootstrap_samples(start = 20, # starting sample size
  stop = 100, # stopping sample size
  increase = 5, # increase bootstrapped samples by this amount
  population = pops, # population or pilot data
  replace = TRUE, # bootstrap with replacement? 
  nsim = 5, # number of simulations to run
  grouping_items = "item") # item column label  

head(samples[[1]])
```

*Step 4 and 5*. The proportion of bootstrapped items across sample sizes below the cutoff score can then be calculated using `calculate_proportion()`. This function returns a dataframe including each sample size with the proportion of items below that cutoff to use in the next function. The `samples` and `cutoff` arguments were previously calculated with our functions. The column for item labels and dependent variables are included as `grouping_items` and `score` arguments to ensure the right calculations.

```{r calc-prop, include = T, echo = T}
proportion_summary <- calculate_proportion(samples = samples, # samples list
  cutoff = cutoff$cutoff, # cut off score 
  grouping_items = "item", # item column name
  score = "score") # dependent variable column name 

head(proportion_summary)
```

*Step 6*. Last, we use the `calculate_correction()` function to correct the sample size scores given the proposed correction formula. The `proportion_summary` from above is used in this function, along with required information about the sample size, proportion of variance from our cutoff calculation, and what power levels should be calculated. Note that the exact percent of items below a cutoff score will be returned if the values in `power_levels` are not exactly calculated. The final summary presents the smallest sample size, corrected, for each of the potential power levels.

```{r calc-correct, include = T, echo = T}
corrected_summary <- calculate_correction(
  proportion_summary = proportion_summary, # prop from above
  pilot_sample_size = 20, # number of participants in the pilot data 
  proportion_variability = cutoff$prop_var, # proportion variance from cutoff scores
  power_levels = c(80, 85, 90, 95)) # what levels of power to calculate 

corrected_summary
```

## Vignettes

While the example in this manuscript was traditionally cognitive linguistics focused, any research using repeated items as a unit of measure could benefit from the proposed newer sampling techniques. Therefore, we provide 12 example vignettes and varied code examples on our OSF page/GitHub site for this manuscript across a range of data types provided by the authors of this manuscript. Examples include psycholinguistics [@dedeyne2008; @heyman2014; @montefinese2022], social psychology data [@ulloa2014], social psychology [@peterson2022; @grahe2022], COVID related data [@montefinese2021], and cognitive psychology [@rer2013; @10.7554/eLife.71601; @barzykowski2019].

# Discussion

We proposed a method combining AIPE, bootstrapping, and simulation to estimate a minimum and maximum sample size and to define a rule for stopping data collection based on narrow confidence intervals on a parameter of interest. In addition, we also demonstrated its practical applications using real-world data. We contend that this procedure is specifically useful for studies with multiple items that intend on using item level focused analyses; furthermore, the utility of measuring each item well can extend to many analysis choices. By focusing on collecting quality data, we can suggest that the data is useful, regardless of the outcome of any hypothesis test.

One limitation of these methods would be our decision to use datasets with very large numbers of items to simulate what might happen within one study. For example, the English Lexicon Project includes thousands of items, and if we were to simulate for all of those, our results would likely suggest needing thousands of participants for most items to reach the criterion. Additionally, as the number of items increases, you may also see very small estimates for sample size due to the correction factor (as with large numbers of items, you could find many items with standard errors below the 40% decile). Therefore, it would be beneficial to consider only simulating what a participant would reasonably complete in a study. Small numbers of repeated items usually result in larger sample sizes proposed from the original pilot data. This result occurs because the smaller number of items means more samples for nearly all to reach the cutoff criteria. These results are similar to what we might expect for a power analysis using a multilevel model - larger numbers of items tend to decrease necessary sample size, while smaller numbers of items tend to increase sample size.

Second, these methods do not ensure the normal interpretation of power, where you know that you would find a specific effect for a specific test, $\alpha$, and so on. As discussed in the Introduction, there is not necessarily a one-to-one mapping of hypothesis to analysis; many of the estimations within a traditional power analysis are just that - best approximations for various parameters. These proposed methods and traditional power analysis could be used together to strengthen our understanding of the sample size necessary for both a hypothesis test and a well-tuned estimation.

Researchers should consider this hybrid approach for AIPE, bootstrapping, and simulation as a powerful tool for hypothesis testing and parameter estimation. This procedure holds benefits for various research studies, specifically replication studies, that usually prioritize subject sample size but rarely item sample size, in spite of the fact that item sample sizes can contribute to power in multilevel models [@brysbaert2018]. Replicated effects, accumulated through multiple studies, contribute to robust meta-analyses, enhancing our understanding of the genuine nature of observed effects. This article helps to achieve this goal by encouraging researchers to conduct studies where the power analysis is not based on the size of the effect but on adequate sampling of the stimuli. We argue that this article can be the initial step to apply AIPE in a manner that can allow researchers to use item information to provide a more accurate and statistically reliable measure of the effect we aimed to investigate. In conclusion, item power analysis is a tool to avoid the waste of resources while ensuring that adequately measured items can be achieved. Well measured data can enable us to counteract the literature that contains false positives, allowing us to achieve replicable, high-quality science to establish answers to scientific questions with precision and accuracy.

\newpage

# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

```{=tex}
\endgroup
```
